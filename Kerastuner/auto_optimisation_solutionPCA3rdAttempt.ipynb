{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "# Provide the correct file path for our training file and our file to be predicted\n",
    "data_file_path = Path('../Resources/PeterMac_HRD_Validation.csv')\n",
    "predict_data_file_path = ('../Resources/PeterMac_HRD_Clinical/correct_PeterMac_HRD_clinical_data.csv')\n",
    "\n",
    "data_df = pd.read_csv(data_file_path)\n",
    "data_df.head()\n",
    "\n",
    "data_df_predict = pd.read_csv(predict_data_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial ID columns, SampleID, SeqRunID, DDMSampleID\n",
    "# Make these columns into One hot columns QAStatus, Gene, Variant\n",
    "# Make a Y value column we actually want to test \n",
    "# testing if there is a difference between MyriadGIStatus and SophiaGIStatus\n",
    "# Non-agreement \n",
    "# As we are testing the agreement of MyriadGiStatus(1/2) and SophiaGIStatus(1/2/3/4), we need to \n",
    "# collapse the test into a binary result.\n",
    "#    MyriadGIStatus        SophiaGIStatus      Laboratory Classsification  Binary Result\n",
    "#       1                       1                   TruePositive                1\n",
    "#       1                       2                   False Negative              0\n",
    "#       1                       3                   Inconclusive                0\n",
    "#       1                       4                   Inconclusive                0\n",
    "#       2                       1                   False Positive              0\n",
    "#       2                       2                   True Negative               1\n",
    "#       2                       3                   Inconclusive                0\n",
    "#       2                       4                   Inconclusive                0\n",
    "# Remove columns for X sample MyriadGIStatus, SophiaGIStatus, MyriadGIScore, SophiaGIIndex\n",
    "# Non-agreement\n",
    "# the Y value will be Non-agreement\n",
    "# We have to decide how to handle columns with actual non-values\n",
    "# being PurityPloidyRatio, Variant. Our lack of records and sparse grouping \n",
    "# makes these columns liklely to be dropped.\n",
    "#  YOUR CODE GOES HERE\n",
    "columns_to_drop = [\"SampleID\", \"SeqRunID\", \"DDMSampleID\",\"MonthsOld\"]\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "data_df = data_df.drop(columns=columns_to_drop, axis=1)\n",
    "data_df.head()\n",
    "\n",
    "data_df_predict = data_df_predict.drop(columns=columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run                  numeric- 13 unique value(s)\n",
      "Source                                   (Data Type: object) - 4 unique value(s)\n",
      "Purity                                   (Data Type: object) - 14 unique value(s)\n",
      "MIDS                 numeric- 24 unique value(s)\n",
      "TotalReads(M)        numeric- 95 unique value(s)\n",
      "lpWGSReads(M)        numeric- 86 unique value(s)\n",
      "TargetPanelReads(M)  numeric- 64 unique value(s)\n",
      "%ReadslpWGS                              (Data Type: object) - 36 unique value(s)\n",
      "%ReadsPanel                              (Data Type: object) - 36 unique value(s)\n",
      "1000x                                    (Data Type: object) - 65 unique value(s)\n",
      "500x                                     (Data Type: object) - 56 unique value(s)\n",
      "200x                                     (Data Type: object) - 32 unique value(s)\n",
      "100x                                     (Data Type: object) - 16 unique value(s)\n",
      "50x                                      (Data Type: object) - 10 unique value(s)\n",
      "25x                                      (Data Type: object) - 6 unique value(s)\n",
      "DupFrac                                  (Data Type: object) - 36 unique value(s)\n",
      "LowCovRegions        numeric- 51 unique value(s)\n",
      "PurityPloidyRatio                        (Data Type: object) - 20 unique value(s)\n",
      "ResNoise                                 (Data Type: object) - 23 unique value(s)\n",
      "SignalNoiseRatio                         (Data Type: object) - 113 unique value(s)\n",
      "QAStatus                                 (Data Type: object) - 3 unique value(s)\n",
      "Gene                                     (Data Type: object) - 4 unique value(s)\n",
      "Variant                                  (Data Type: object) - 39 unique value(s)\n",
      "%VariantFraction                         (Data Type: object) - 39 unique value(s)\n",
      "MyriadGIScore        numeric- 70 unique value(s)\n",
      "MyriadGIStatus       numeric- 2 unique value(s)\n",
      "SOPHiAGIIndex                            (Data Type: object) - 105 unique value(s)\n",
      "SophiaGIStatus       numeric- 4 unique value(s)\n"
     ]
    }
   ],
   "source": [
    "# Finding attribute columns\n",
    "application_categories = data_df.dtypes[data_df.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "# Check the number of unique values in each column of object type\n",
    "columns = data_df[application_categories].nunique()\n",
    "\n",
    "# Iterate through the columns and print the unique value counts for each column\n",
    "# we iterate through every column in the dataframe, some of them of object type where we found the unique count\n",
    "for column in data_df.columns:\n",
    "    if column in columns.index:\n",
    "        data_type = data_df[column].dtype\n",
    "        print(f\"{column.ljust(40)} (Data Type: {data_type}) - {columns[column]} unique value(s)\")\n",
    "    else:\n",
    "        print(f\"{column.ljust(20)} numeric- {data_df[column].nunique()} unique value(s)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity    object\n",
      "dtype: object\n",
      "Purity    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert columns we will not bin, from pobject type to numeric\n",
    "column_names_to_convert = ['Purity']\n",
    "\n",
    "# Step 1: Check the current data types of the columns\n",
    "print(data_df[column_names_to_convert].dtypes)\n",
    "\n",
    "# Step 2: Convert each column to numeric (if possible)\n",
    "for col in column_names_to_convert:\n",
    "    data_df[col] = pd.to_numeric(data_df[col], errors='coerce')\n",
    "    data_df_predict[col] = pd.to_numeric(data_df_predict[col], errors='coerce') \n",
    "# Step 3: Check the new data types of the columns after the conversion\n",
    "print(data_df[column_names_to_convert].dtypes)\n",
    "# print(data_df[\"Source\"].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['81%' '76%' '64%' '63%' '58%' '54%' '90%' '89%' '84%' '85%' '98%' '72%'\n",
      " '68%' '69%' '74%' '66%' '73%' '70%' '80%' '86%' '15%' '91%' '62%' '79%'\n",
      " '88%' '92%' '71%' '77%' '59%' '60%' '55%' '65%' '67%' '57%' '61%' '75%']\n",
      "['19%' '24%' '36%' '37%' '42%' '46%' '10%' '12%' '16%' '15%' '2%' '28%'\n",
      " '32%' '31%' '26%' '29%' '34%' '25%' '30%' '20%' '14%' '85%' '9%' '38%'\n",
      " '21%' '11%' '8%' '23%' '41%' '40%' '27%' '45%' '35%' '33%' '43%' '39%']\n"
     ]
    }
   ],
   "source": [
    "print(data_df['%ReadslpWGS'].unique())\n",
    "print(data_df['%ReadsPanel'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "data_df['PurityPloidyRatio'] = data_df['PurityPloidyRatio'].replace('-', 0.0)\n",
    "data_df['ResNoise'] = data_df['ResNoise'].replace('-', 0.0)\n",
    "data_df['SignalNoiseRatio'] = data_df['SignalNoiseRatio'].replace('-', 0.0)\n",
    "\n",
    "data_df['Gene'] = data_df['Gene'].replace('.', 'Unlisted')\n",
    "data_df_predict['Gene'] = data_df_predict['Gene'].fillna('Unlisted')\n",
    "\n",
    "# data_df['MonthsOld'] = data_df['MonthsOld'].fillna(0.0)\n",
    "# data_df['MonthsOld'] = data_df['MonthsOld'].replace('.', 0.0)\n",
    "data_df['Purity'] = data_df['Purity'].replace('.', 0.0)\n",
    "data_df['%VariantFraction'] = data_df['%VariantFraction'].replace('.', 0.0)\n",
    "data_df_predict['%VariantFraction'] = data_df_predict['%VariantFraction'].replace('',0.0)\n",
    "# Convert the 'Purity' column to numeric, replacing '.' with 0.0\n",
    "data_df['Purity'] = pd.to_numeric(data_df['Purity'], errors='coerce').fillna(0.0)\n",
    "\n",
    "data_df['DupFrac'] = data_df['DupFrac'].replace('%', '', regex=True).astype(float)\n",
    "data_df['%ReadslpWGS'] = data_df['%ReadslpWGS'].replace('%', '', regex=True).astype(float)\n",
    "data_df['%ReadsPanel'] = data_df['%ReadsPanel'].replace('%', '', regex=True).astype(float)\n",
    "\n",
    "data_df['Variant'] = data_df['Variant'].replace('.', 'Unlisted')\n",
    "\n",
    "data_df_predict['Variant'] = data_df_predict['Variant'].fillna('Unlisted')\n",
    "# Apply label encoding to 'Variant' column\n",
    "label_encoder = LabelEncoder()\n",
    "data_df['Variant'] = label_encoder.fit_transform(data_df['Variant'].astype(str))\n",
    "data_df_predict['Variant'] = label_encoder.fit_transform(data_df_predict['Variant'].astype(str))\n",
    "\n",
    "data_df_predict['Run'] = data_df_predict['Run'].str.replace('Run', '')\n",
    "data_df_predict['MIDS'] = data_df_predict['MIDS'].str.replace('S', '')\n",
    "\n",
    "data_df['1000x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "\n",
    "data_df['500x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "data_df['200x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "data_df['100x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "data_df['50x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "data_df['25x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "\n",
    "data_df_predict['MyriadGIScore'] = '0'\n",
    "data_df_predict['MyriadGIStatus'] = '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Run  Source  Purity  MIDS  TotalReads(M)  lpWGSReads(M)  \\\n",
      "0      1      AZ    20.0     2            7.3            5.9   \n",
      "1      1      AZ    30.0     3            7.3            5.6   \n",
      "2      1      AZ    20.0     4            9.6            6.1   \n",
      "3      1      AZ    20.0     6            8.9            5.6   \n",
      "4      1      AZ    60.0     7            8.6            5.0   \n",
      "..   ...     ...     ...   ...            ...            ...   \n",
      "134    6      AZ    10.0     3           25.5           18.8   \n",
      "135   11  GREECE     0.0     4           18.6           13.0   \n",
      "136    2      AZ    40.0    23           18.3           12.2   \n",
      "137    5      AZ    50.0    23           14.0           10.5   \n",
      "138   12  BRAZIL    60.0    23           21.6           15.5   \n",
      "\n",
      "     TargetPanelReads(M)  %ReadslpWGS  %ReadsPanel  1000x  ...  \\\n",
      "0                    1.4         81.0         19.0    1.0  ...   \n",
      "1                    1.7         76.0         24.0    2.0  ...   \n",
      "2                    3.5         64.0         36.0   41.0  ...   \n",
      "3                    3.3         63.0         37.0   16.0  ...   \n",
      "4                    3.6         58.0         42.0    2.0  ...   \n",
      "..                   ...          ...          ...    ...  ...   \n",
      "134                  6.7         74.0         26.0   88.0  ...   \n",
      "135                  5.6         70.0         30.0    0.0  ...   \n",
      "136                  6.1         66.0         34.0   10.0  ...   \n",
      "137                  3.5         75.0         25.0    1.0  ...   \n",
      "138                  6.1         72.0         28.0   82.0  ...   \n",
      "\n",
      "     SignalNoiseRatio  QAStatus      Gene  Variant  %VariantFraction  \\\n",
      "0                2.95    Medium  Unlisted        0               0.0   \n",
      "1                2.91      High  Unlisted        0               0.0   \n",
      "2                1.64      High  Unlisted        0               0.0   \n",
      "3                3.49      High  Unlisted        0               0.0   \n",
      "4                2.18      High  Unlisted        0               0.0   \n",
      "..                ...       ...       ...      ...               ...   \n",
      "134              1.08    Medium  Unlisted        0               0.0   \n",
      "135               0.4    Medium  Unlisted        0               0.0   \n",
      "136              1.56    Medium  Unlisted        0               0.0   \n",
      "137               1.7    Medium     BRCA2       26              26.7   \n",
      "138               1.1    Medium     BRCA1       25              64.7   \n",
      "\n",
      "     MyriadGIScore  MyriadGIStatus SOPHiAGIIndex SophiaGIStatus Non-agreement  \n",
      "0               51               1           3.2              1             1  \n",
      "1               20               2         -15.7              2             1  \n",
      "2               17               2          -4.6              2             1  \n",
      "3               29               2          -4.6              2             1  \n",
      "4               29               2          -8.2              2             1  \n",
      "..             ...             ...           ...            ...           ...  \n",
      "134              3               2            -9              2             1  \n",
      "135              7               2             -              3             0  \n",
      "136             38               2          -4.9              2             1  \n",
      "137             25               2          -1.4              2             1  \n",
      "138             31               2          -2.7              2             1  \n",
      "\n",
      "[139 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "def calculate_non_agreement(row):\n",
    "    if row['MyriadGIStatus'] == 1 and row['SophiaGIStatus'] == 1:\n",
    "        return 1\n",
    "    elif row['MyriadGIStatus'] == 1 and row['SophiaGIStatus'] == 2:\n",
    "        return 0\n",
    "    elif row['MyriadGIStatus'] == 1 and row['SophiaGIStatus'] == 3:\n",
    "        return 0\n",
    "    elif row['MyriadGIStatus'] == 1 and row['SophiaGIStatus'] == 4:\n",
    "        return 0\n",
    "    elif row['MyriadGIStatus'] == 2 and row['SophiaGIStatus'] == 1:\n",
    "        return 0\n",
    "    elif row['MyriadGIStatus'] == 2 and row['SophiaGIStatus'] == 2:\n",
    "        return 1\n",
    "    elif row['MyriadGIStatus'] == 2 and row['SophiaGIStatus'] == 3:\n",
    "        return 0\n",
    "    elif row['MyriadGIStatus'] == 2 and row['SophiaGIStatus'] == 4:\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the 'Non-agreement' column\n",
    "data_df['Non-agreement'] = data_df.apply(calculate_non_agreement, axis=1)\n",
    "data_df_predict['Non-agreement'] = 'void'\n",
    "#print(data_df['Non-agreement'].dtypes)\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Run  Source  Purity  MIDS  TotalReads(M)  lpWGSReads(M)  \\\n",
      "0      1      AZ    20.0     2            7.3            5.9   \n",
      "1      1      AZ    30.0     3            7.3            5.6   \n",
      "2      1      AZ    20.0     4            9.6            6.1   \n",
      "3      1      AZ    20.0     6            8.9            5.6   \n",
      "4      1      AZ    60.0     7            8.6            5.0   \n",
      "..   ...     ...     ...   ...            ...            ...   \n",
      "134    6      AZ    10.0     3           25.5           18.8   \n",
      "135   11  GREECE     0.0     4           18.6           13.0   \n",
      "136    2      AZ    40.0    23           18.3           12.2   \n",
      "137    5      AZ    50.0    23           14.0           10.5   \n",
      "138   12  BRAZIL    60.0    23           21.6           15.5   \n",
      "\n",
      "     TargetPanelReads(M)  %ReadslpWGS  %ReadsPanel  1000x  ...  \\\n",
      "0                    1.4         81.0         19.0    1.0  ...   \n",
      "1                    1.7         76.0         24.0    2.0  ...   \n",
      "2                    3.5         64.0         36.0   41.0  ...   \n",
      "3                    3.3         63.0         37.0   16.0  ...   \n",
      "4                    3.6         58.0         42.0    2.0  ...   \n",
      "..                   ...          ...          ...    ...  ...   \n",
      "134                  6.7         74.0         26.0   88.0  ...   \n",
      "135                  5.6         70.0         30.0    0.0  ...   \n",
      "136                  6.1         66.0         34.0   10.0  ...   \n",
      "137                  3.5         75.0         25.0    1.0  ...   \n",
      "138                  6.1         72.0         28.0   82.0  ...   \n",
      "\n",
      "     SignalNoiseRatio  QAStatus      Gene  Variant  %VariantFraction  \\\n",
      "0                2.95    Medium  Unlisted        0               0.0   \n",
      "1                2.91      High  Unlisted        0               0.0   \n",
      "2                1.64      High  Unlisted        0               0.0   \n",
      "3                3.49      High  Unlisted        0               0.0   \n",
      "4                2.18      High  Unlisted        0               0.0   \n",
      "..                ...       ...       ...      ...               ...   \n",
      "134              1.08    Medium  Unlisted        0               0.0   \n",
      "135               0.4    Medium  Unlisted        0               0.0   \n",
      "136              1.56    Medium  Unlisted        0               0.0   \n",
      "137               1.7    Medium     BRCA2       26              26.7   \n",
      "138               1.1    Medium     BRCA1       25              64.7   \n",
      "\n",
      "     MyriadGIScore  MyriadGIStatus SOPHiAGIIndex SophiaGIStatus Non-agreement  \n",
      "0               51               1           3.2              1             1  \n",
      "1               20               2         -15.7              2             1  \n",
      "2               17               2          -4.6              2             1  \n",
      "3               29               2          -4.6              2             1  \n",
      "4               29               2          -8.2              2             1  \n",
      "..             ...             ...           ...            ...           ...  \n",
      "134              3               2            -9              2             1  \n",
      "135              7               2             -              3             0  \n",
      "136             38               2          -4.9              2             1  \n",
      "137             25               2          -1.4              2             1  \n",
      "138             31               2          -2.7              2             1  \n",
      "\n",
      "[139 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  QAStatus  COUNT\n",
      "0     High     89\n",
      "1   Medium     42\n",
      "2      Low      8\n",
      "---\n",
      "       Gene  COUNT\n",
      "0  Unlisted     99\n",
      "1     BRCA1     21\n",
      "2     BRCA2     18\n",
      "3    RAD51D      1\n",
      "---\n",
      "    Variant  COUNT\n",
      "0         0     99\n",
      "1        14      2\n",
      "2        21      2\n",
      "3        36      1\n",
      "4        27      1\n",
      "5        22      1\n",
      "6        23      1\n",
      "7        24      1\n",
      "8        25      1\n",
      "9        37      1\n",
      "10       26      1\n",
      "11       28      1\n",
      "12       35      1\n",
      "13       20      1\n",
      "14       30      1\n",
      "15       31      1\n",
      "16       32      1\n",
      "17       33      1\n",
      "18       34      1\n",
      "19       29      1\n",
      "20       19      1\n",
      "21        1      1\n",
      "22       18      1\n",
      "23        2      1\n",
      "24        3      1\n",
      "25        4      1\n",
      "26        5      1\n",
      "27        6      1\n",
      "28        7      1\n",
      "29        8      1\n",
      "30        9      1\n",
      "31       10      1\n",
      "32       11      1\n",
      "33       12      1\n",
      "34       13      1\n",
      "35       15      1\n",
      "36       16      1\n",
      "37       17      1\n",
      "38       38      1\n",
      "---\n",
      "   PurityPloidyRatio  COUNT\n",
      "0                0.0     46\n",
      "1               0.23     15\n",
      "2               0.25     11\n",
      "3               0.45      9\n",
      "4                0.3      8\n",
      "5               0.28      7\n",
      "6               0.33      6\n",
      "7                0.2      5\n",
      "8               0.35      5\n",
      "9               0.38      5\n",
      "10              0.15      4\n",
      "11                 0      3\n",
      "12              0.42      3\n",
      "13              0.47      3\n",
      "14              0.17      2\n",
      "15              0.12      2\n",
      "16               0.4      2\n",
      "17               0.1      1\n",
      "18              0.07      1\n",
      "19               0.5      1\n"
     ]
    }
   ],
   "source": [
    "# Look at APPLICATION_TYPE value counts for binning  QAStatus\n",
    "\n",
    "grouped_df = data_df.groupby(\"QAStatus\").size().reset_index(name=\"COUNT\")\n",
    "sorted_df = grouped_df.sort_values(by=\"COUNT\", ascending=False)\n",
    "sorted_df = sorted_df.reset_index(drop=True)\n",
    "print(sorted_df)\n",
    "print('---')\n",
    "grouped_df = data_df.groupby(\"Gene\").size().reset_index(name=\"COUNT\")\n",
    "sorted_df = grouped_df.sort_values(by=\"COUNT\", ascending=False)\n",
    "sorted_df = sorted_df.reset_index(drop=True)\n",
    "print(sorted_df)\n",
    "print('---')\n",
    "grouped_df = data_df.groupby(\"Variant\").size().reset_index(name=\"COUNT\")\n",
    "sorted_df = grouped_df.sort_values(by=\"COUNT\", ascending=False)\n",
    "sorted_df = sorted_df.reset_index(drop=True)\n",
    "print(sorted_df)\n",
    "print('---')\n",
    "grouped_df = data_df.groupby(\"PurityPloidyRatio\").size().reset_index(name=\"COUNT\")\n",
    "sorted_df = grouped_df.sort_values(by=\"COUNT\", ascending=False)\n",
    "sorted_df = sorted_df.reset_index(drop=True)\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dummy block of code in case we want to gather many column values into the same bucket\n",
    "# application_types_to_replace = [\"T9\", \"T13\", \"T12\", \"T2\", \"T14\", \"T25\", \"T29\", \"T15\", \"T17\"]\n",
    "\n",
    "# Replace the specified values in the \"APPLICATION_TYPE\" column with \"Other\"\n",
    "# data_df['APPLICATION_TYPE'] = data_df['APPLICATION_TYPE'].replace(application_types_to_replace, \"Other\")\n",
    "\n",
    "# Check the value counts after replacing\n",
    "# print(data_df['APPLICATION_TYPE'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run                      int64\n",
      "Source                  object\n",
      "Purity                 float64\n",
      "MIDS                     int64\n",
      "TotalReads(M)          float64\n",
      "lpWGSReads(M)          float64\n",
      "TargetPanelReads(M)    float64\n",
      "%ReadslpWGS            float64\n",
      "%ReadsPanel            float64\n",
      "1000x                  float64\n",
      "500x                   float64\n",
      "200x                   float64\n",
      "100x                   float64\n",
      "50x                    float64\n",
      "25x                    float64\n",
      "DupFrac                float64\n",
      "LowCovRegions            int64\n",
      "PurityPloidyRatio       object\n",
      "ResNoise                object\n",
      "SignalNoiseRatio        object\n",
      "QAStatus                object\n",
      "Gene                    object\n",
      "Variant                  int64\n",
      "%VariantFraction        object\n",
      "MyriadGIScore            int64\n",
      "MyriadGIStatus           int64\n",
      "SOPHiAGIIndex           object\n",
      "SophiaGIStatus           int64\n",
      "Non-agreement            int64\n",
      "Variant                  int64\n",
      "QAStatus_High            uint8\n",
      "QAStatus_Low             uint8\n",
      "QAStatus_Medium          uint8\n",
      "Gene_BRCA1               uint8\n",
      "Gene_BRCA2               uint8\n",
      "Gene_RAD51D              uint8\n",
      "Gene_Unlisted            uint8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create one-hot columns for these 3 columns\n",
    "onehot_cols = [\"QAStatus\", \"Gene\", \"Variant\"]  #\"PurityPloidyRatio\"]\n",
    "\n",
    "# Use get_dummies() to one-hot encode only the categorical columns\n",
    "one_hot_encoded = pd.get_dummies(data_df[onehot_cols])\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "data_df = pd.concat([data_df, one_hot_encoded], axis=1)\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(data_df_predict[onehot_cols])\n",
    "data_df_predict = pd.concat([data_df_predict, one_hot_encoded], axis=1 )\n",
    "# After this, you can print the data types of columns in the 'data_df' DataFrame\n",
    "column_types = data_df.dtypes\n",
    "print(column_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_callback(message):\n",
    "    print(f\"Final callback: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_accuracy_callback(epoch, loss, accuracy):\n",
    "    print(f\"Epoch {epoch}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from datetime import datetime\n",
    "\n",
    "def main_process(callback, X_train_scaled, y_train, nn, report_interval=5):\n",
    "    # Train the model and store the training history\n",
    "    fit_model = nn.fit(X_train_scaled, y_train, epochs=100, verbose=0, callbacks=[callback])  # Pass the callback here\n",
    "    training_history = fit_model.history\n",
    "    \n",
    "    print(\"Training has started.\")\n",
    "    epochs = 100\n",
    "    \n",
    "    # Report loss and accuracy at the specified intervals\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        if epoch % report_interval == 0 or epoch == epochs:\n",
    "            loss = training_history['loss'][epoch - 1]\n",
    "            accuracy = training_history['accuracy'][epoch - 1]\n",
    "            print(f\"Epoch {epoch}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\")#soos2\n",
    "            callback.on_epoch_end(epoch, {'loss': loss, 'accuracy': accuracy})  # Manually call the on_epoch_end method\n",
    "\n",
    "    result = \"Task completed.\"\n",
    "    # final_callback(result)  # Comment out or remove this line as it is not defined in the code\n",
    "    print(\"Main process finished.\")\n",
    "\n",
    "# soos\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "script_name = \"Starter_Codev1\"\n",
    "csv_loss_accuracy_file = \"startercodetest.csv\"\n",
    "#csv_loss_accuracy_file_name = f\"{script_name}_test_result_{current_datetime}.csv\"\n",
    "\n",
    "# Create the CSVLogger callback and pass the file name\n",
    "csv_logger = CSVLogger(csv_loss_accuracy_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "### In our original model, we created data frame application_df, which still exists.\n",
    "- 1. I will create a pca method on this data frame which has had bucketing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Run', 'Source', 'Purity', 'MIDS', 'TotalReads(M)', 'lpWGSReads(M)',\n",
       "       'TargetPanelReads(M)', '%ReadslpWGS', '%ReadsPanel', '1000x', '500x',\n",
       "       '200x', '100x', '50x', '25x', 'DupFrac', 'LowCovRegions',\n",
       "       'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio', 'Variant',\n",
       "       '%VariantFraction', 'Variant', 'QAStatus_High', 'QAStatus_Low',\n",
       "       'QAStatus_Medium', 'Gene_BRCA1', 'Gene_BRCA2', 'Gene_RAD51D',\n",
       "       'Gene_Unlisted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1  pca method on application_df which has had bucketing performed for the original model.\n",
    "data_df_x = data_df.copy() \n",
    "columns_to_drop = [\"Gene\", \"Non-agreement\", \"MyriadGIStatus\", \"SophiaGIStatus\", \"MyriadGIScore\", \"SOPHiAGIIndex\", \"QAStatus\"]\n",
    "data_df_x = data_df_x.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "data_df_predict_x = data_df_predict.copy()\n",
    "data_df_predict_x = data_df_predict_x.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "#data_df_x = data_df_x.drop(columns=[\"Gene\",\"Source\",\"Non_agreement\", \"MyriadGIStatus\", \"SophiaGIStatus\", \"MyriadGIScore\", \"SOPHiAGIIndex\",\"QAStatus\"], axis=1)\n",
    "data_df_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Run', 'Source', 'Purity', 'MIDS', 'TotalReads(M)', 'lpWGSReads(M)',\n",
       "       'TargetPanelReads(M)', '%ReadslpWGS', '%ReadsPanel', '1000x', '500x',\n",
       "       '200x', '100x', '50x', '25x', 'DupFrac', 'LowCovRegions',\n",
       "       'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio', 'Variant',\n",
       "       '%VariantFraction', 'Variant', 'QAStatus_High', 'QAStatus_Medium',\n",
       "       'Gene_BRCA1', 'Gene_BRCA2', 'Gene_Unlisted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_predict_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: bool)\n"
     ]
    }
   ],
   "source": [
    "#data_df_x\n",
    "# Step 1: Check which columns have missing values (NaN)\n",
    "columns_with_missing_values = data_df_x.isna().any()\n",
    "\n",
    "# Step 2: Output the columns where the count of missing values is larger than 0\n",
    "columns_with_missing_values = columns_with_missing_values[columns_with_missing_values]\n",
    "\n",
    "# Output the columns with missing values\n",
    "print(columns_with_missing_values)\n",
    "#print(data_df_x['%ReadslpWGS'].unique())\n",
    "#print(data_df_x['%ReadsPanel'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0 '42.6' '78.1' '78.5' '77.2' '22.1' '37.8' '56.4' '24.4' '87' '54.6'\n",
      " '51.1' '85.8' '71.8' '66.1' '9' '87.4' '84.1' '80.4' '73' '67.6' '17.3'\n",
      " '9.1' '35.8' '82.8' '80.5' 'Deleted' '18.7' '65.3' '51.8' '68.8' '62.6'\n",
      " '71.2' '89.8' '47' '59.6' '5.4' '26.7' '64.7']\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the \"%VariantFraction\" column\n",
    "print(data_df['%VariantFraction'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Deleted\" with 0 in the '%ReadslpWGS' column\n",
    "data_df_x['%VariantFraction'] = data_df_x['%VariantFraction'].replace('Deleted', 0)\n",
    "\n",
    "# Convert the column to numeric (float) format\n",
    "data_df_x['%VariantFraction'] = pd.to_numeric(data_df_x['%VariantFraction'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns:\n",
      "Index(['Source', 'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#data_df_x.columns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data_df_x' is the DataFrame containing the data\n",
    "non_numeric_columns = data_df_x.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "print(\"Non-numeric columns:\")\n",
    "print(non_numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Source', 'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "non_numeric_columns = data_df_x.select_dtypes(exclude=[np.number]).columns\n",
    "print(non_numeric_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run                      int64\n",
      "Source                  object\n",
      "Purity                 float64\n",
      "MIDS                     int64\n",
      "TotalReads(M)          float64\n",
      "lpWGSReads(M)          float64\n",
      "TargetPanelReads(M)    float64\n",
      "%ReadslpWGS            float64\n",
      "%ReadsPanel            float64\n",
      "1000x                  float64\n",
      "500x                   float64\n",
      "200x                   float64\n",
      "100x                   float64\n",
      "50x                    float64\n",
      "25x                    float64\n",
      "DupFrac                float64\n",
      "LowCovRegions            int64\n",
      "PurityPloidyRatio       object\n",
      "ResNoise                object\n",
      "SignalNoiseRatio        object\n",
      "Variant                  int64\n",
      "%VariantFraction       float64\n",
      "Variant                  int64\n",
      "QAStatus_High            uint8\n",
      "QAStatus_Low             uint8\n",
      "QAStatus_Medium          uint8\n",
      "Gene_BRCA1               uint8\n",
      "Gene_BRCA2               uint8\n",
      "Gene_RAD51D              uint8\n",
      "Gene_Unlisted            uint8\n",
      "dtype: object\n",
      "Run                    0\n",
      "Source                 0\n",
      "Purity                 0\n",
      "MIDS                   0\n",
      "TotalReads(M)          0\n",
      "lpWGSReads(M)          0\n",
      "TargetPanelReads(M)    0\n",
      "%ReadslpWGS            0\n",
      "%ReadsPanel            0\n",
      "1000x                  0\n",
      "500x                   0\n",
      "200x                   0\n",
      "100x                   0\n",
      "50x                    0\n",
      "25x                    0\n",
      "DupFrac                0\n",
      "LowCovRegions          0\n",
      "PurityPloidyRatio      0\n",
      "ResNoise               0\n",
      "SignalNoiseRatio       0\n",
      "Variant                0\n",
      "%VariantFraction       0\n",
      "Variant                0\n",
      "QAStatus_High          0\n",
      "QAStatus_Low           0\n",
      "QAStatus_Medium        0\n",
      "Gene_BRCA1             0\n",
      "Gene_BRCA2             0\n",
      "Gene_RAD51D            0\n",
      "Gene_Unlisted          0\n",
      "dtype: int64\n",
      "Run                      0\n",
      "Source                 139\n",
      "Purity                   0\n",
      "MIDS                     0\n",
      "TotalReads(M)            0\n",
      "lpWGSReads(M)            0\n",
      "TargetPanelReads(M)      0\n",
      "%ReadslpWGS              0\n",
      "%ReadsPanel              0\n",
      "1000x                    0\n",
      "500x                     0\n",
      "200x                     0\n",
      "100x                     0\n",
      "50x                      0\n",
      "25x                      0\n",
      "DupFrac                  0\n",
      "LowCovRegions            0\n",
      "PurityPloidyRatio        0\n",
      "ResNoise                 0\n",
      "SignalNoiseRatio         0\n",
      "Variant                  0\n",
      "%VariantFraction         0\n",
      "Variant                  0\n",
      "QAStatus_High            0\n",
      "QAStatus_Low             0\n",
      "QAStatus_Medium          0\n",
      "Gene_BRCA1               0\n",
      "Gene_BRCA2               0\n",
      "Gene_RAD51D              0\n",
      "Gene_Unlisted            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check data types of the columns in data_df_x\n",
    "print(data_df_x.dtypes)\n",
    "\n",
    "# Check for missing values in data_df_x\n",
    "print(data_df_x.isnull().sum())\n",
    "\n",
    "# Convert the '%VariantFraction' column to numeric values, invalid values will be converted to NaN\n",
    "data_df_x['%VariantFraction'] = pd.to_numeric(data_df_x['%VariantFraction'], errors='coerce')\n",
    "\n",
    "# Create a mask to identify rows with NaN values in the '%VariantFraction' column\n",
    "invalid_rows_mask = data_df_x['%VariantFraction'].isna()\n",
    "\n",
    "# Use the mask to filter the DataFrame and get the rows with invalid values\n",
    "invalid_rows = data_df_x[invalid_rows_mask]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if any non-numeric values still exist in data_df_x\n",
    "non_numeric_values = data_df_x.apply(pd.to_numeric, errors='coerce').isnull().sum()\n",
    "print(non_numeric_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: Index(['Source', 'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio'], dtype='object')\n",
      "Missing values:\n",
      " Run                    0\n",
      "Source                 0\n",
      "Purity                 0\n",
      "MIDS                   0\n",
      "TotalReads(M)          0\n",
      "lpWGSReads(M)          0\n",
      "TargetPanelReads(M)    0\n",
      "%ReadslpWGS            0\n",
      "%ReadsPanel            0\n",
      "1000x                  0\n",
      "500x                   0\n",
      "200x                   0\n",
      "100x                   0\n",
      "50x                    0\n",
      "25x                    0\n",
      "DupFrac                0\n",
      "LowCovRegions          0\n",
      "PurityPloidyRatio      0\n",
      "ResNoise               0\n",
      "SignalNoiseRatio       0\n",
      "Variant                0\n",
      "%VariantFraction       0\n",
      "Variant                0\n",
      "QAStatus_High          0\n",
      "QAStatus_Low           0\n",
      "QAStatus_Medium        0\n",
      "Gene_BRCA1             0\n",
      "Gene_BRCA2             0\n",
      "Gene_RAD51D            0\n",
      "Gene_Unlisted          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for non-numeric columns\n",
    "non_numeric_cols = data_df_x.select_dtypes(exclude=[np.number]).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data_df_x.isnull().sum()\n",
    "print(\"Missing values:\\n\", missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Run', 'Purity', 'MIDS', 'TotalReads(M)', 'lpWGSReads(M)',\n",
       "       'TargetPanelReads(M)', '%ReadslpWGS', '%ReadsPanel', '1000x', '500x',\n",
       "       '200x', '100x', '50x', '25x', 'DupFrac', 'LowCovRegions',\n",
       "       'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio', '%VariantFraction',\n",
       "       'QAStatus_High', 'QAStatus_Low', 'QAStatus_Medium', 'Gene_BRCA1',\n",
       "       'Gene_BRCA2', 'Gene_RAD51D', 'Gene_Unlisted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['Variant', 'Source']\n",
    "data_df_x = data_df_x.drop(columns=columns_to_drop)\n",
    "\n",
    "data_df_predict_x = data_df_predict_x.drop(columns=columns_to_drop)\n",
    "\n",
    "data_df_x.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_predict_x.columns\n",
    "data_df_predict_x['QAStatus_Low'] = 0\n",
    "data_df_predict_x['Gene_RAD51D'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Run, Purity, MIDS, TotalReads(M), lpWGSReads(M), TargetPanelReads(M), %ReadslpWGS, %ReadsPanel, 1000x, 500x, 200x, 100x, 50x, 25x, DupFrac, LowCovRegions, PurityPloidyRatio, ResNoise, SignalNoiseRatio, %VariantFraction, QAStatus_High, QAStatus_Low, QAStatus_Medium, Gene_BRCA1, Gene_BRCA2, Gene_RAD51D, Gene_Unlisted]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "invalid_rows = data_df_x['%VariantFraction'].apply(lambda x: not str(x).replace('.', '').isnumeric())\n",
    "\n",
    "# Display the rows containing the invalid values\n",
    "print(data_df_x[invalid_rows])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139, 27)\n",
      "(111, 27) (28, 27)\n",
      "(111, 3) (28, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume data_df_x contains the features, and data_df contains the target \"Non_agreement\" column\n",
    "\n",
    "# Split our preprocessed data into our features and target arrays\n",
    "y2 = data_df[\"Non-agreement\"].values\n",
    "X2 = data_df_x.values\n",
    "X3 = data_df_predict_x.values\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=78)\n",
    "\n",
    "# Create a new scaler instance and fit it on the training data\n",
    "scaler = StandardScaler()\n",
    "X2_train_scaled = scaler.fit_transform(X2_train)\n",
    "\n",
    "# Transform both the training and testing data using the same scaler\n",
    "X2_test_scaled = scaler.transform(X2_test)\n",
    "X3_scaled = scaler.transform(X3)\n",
    "# Now perform PCA on the scaled training data\n",
    "pca = PCA(n_components=3)\n",
    "X2_train_pca = pca.fit_transform(X2_train_scaled)\n",
    "\n",
    "# Apply the same PCA transformation to the scaled testing data\n",
    "X2_test_pca = pca.transform(X2_test_scaled)\n",
    "\n",
    "# Check the number of records in the original dataset\n",
    "print(data_df_x.shape)\n",
    "\n",
    "# Check the number of records after splitting into training and testing sets\n",
    "print(X2_train.shape, X2_test.shape)\n",
    "\n",
    "# Check the number of records after PCA transformation\n",
    "print(X2_train_pca.shape, X2_test_pca.shape)\n",
    "\n",
    "# Create a new DataFrame with the PCA data for both training and testing sets\n",
    "df_train_pca = pd.DataFrame(X2_train_pca, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "df_test_pca = pd.DataFrame(X2_test_pca, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer instance with the strategy as 'constant' and fill_value as 0\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "# Impute missing values in X3_scaled with 0\n",
    "X3_imputed = imputer.fit_transform(X3_scaled)\n",
    "\n",
    "# Perform PCA on the imputed data\n",
    "X3_pca = pca.transform(X3_imputed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation',['relu','tanh','sigmoid'])\n",
    "    \n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=1,\n",
    "        max_value=30,\n",
    "        step=2), activation=activation, input_dim=3))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 6)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=1,\n",
    "            max_value=30,\n",
    "            step=2),\n",
    "            activation=activation))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kerastuner library\n",
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=100,\n",
    "    hyperband_iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 508 Complete [00h 00m 07s]\n",
      "val_accuracy: 0.7142857313156128\n",
      "\n",
      "Best val_accuracy So Far: 0.8571428656578064\n",
      "Total elapsed time: 00h 20m 20s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Note: Use RSoos Scaled train data and test here\n",
    "# Run the kerastuner search for best hyperparameters\n",
    "tuner.search(X2_train_pca,y2_train,epochs=100,validation_data=(X2_test_pca,y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'first_units': 9,\n",
       " 'num_layers': 1,\n",
       " 'units_0': 15,\n",
       " 'units_1': 3,\n",
       " 'units_2': 3,\n",
       " 'units_3': 23,\n",
       " 'units_4': 3,\n",
       " 'units_5': 11,\n",
       " 'tuner/epochs': 12,\n",
       " 'tuner/initial_epoch': 0,\n",
       " 'tuner/bracket': 2,\n",
       " 'tuner/round': 0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get best model hyperparameters\n",
    "best_hyper = tuner.get_best_hyperparameters(3)[0]\n",
    "best_hyper.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.6099 - accuracy: 0.8571 - 225ms/epoch - 225ms/step\n",
      "Loss: 0.6098801493644714, Accuracy: 0.8571428656578064\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model against full test data\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "model_loss, model_accuracy = best_model.evaluate(X2_test_pca,y2_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shared/anaconda3/envs/bootcamp/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save optimized model\n",
    "best_model.save('./Kerastuner/untitled_project/PCA3rdAttempt_optimised_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Perform predictions with best model on clinical data\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "optimized_model = load_model('./Kerastuner/untitled_project/PCA3rdAttempt_optimised_model.h5')\n",
    "predictions = optimized_model.predict(X3_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5615081 ]\n",
      " [0.4854239 ]\n",
      " [0.58651936]\n",
      " [0.566744  ]\n",
      " [0.5992895 ]\n",
      " [0.5167243 ]\n",
      " [0.5854251 ]\n",
      " [0.541069  ]\n",
      " [0.5171153 ]\n",
      " [0.52674395]\n",
      " [0.5543784 ]\n",
      " [0.53281784]\n",
      " [0.5615725 ]\n",
      " [0.5572183 ]\n",
      " [0.53708255]\n",
      " [0.56197864]\n",
      " [0.5010312 ]\n",
      " [0.535943  ]\n",
      " [0.5332915 ]\n",
      " [0.57277566]\n",
      " [0.4383167 ]\n",
      " [0.544989  ]\n",
      " [0.5499791 ]\n",
      " [0.46143505]\n",
      " [0.5345393 ]\n",
      " [0.52490515]\n",
      " [0.50299704]\n",
      " [0.5458166 ]\n",
      " [0.4781204 ]\n",
      " [0.5718261 ]\n",
      " [0.5489444 ]\n",
      " [0.557952  ]\n",
      " [0.48067078]\n",
      " [0.61916554]\n",
      " [0.4833243 ]\n",
      " [0.5262348 ]\n",
      " [0.6229367 ]\n",
      " [0.5567674 ]\n",
      " [0.58706   ]\n",
      " [0.4813462 ]\n",
      " [0.56450397]\n",
      " [0.5056111 ]\n",
      " [0.5526079 ]\n",
      " [0.5664058 ]\n",
      " [0.54611105]\n",
      " [0.53274   ]\n",
      " [0.487293  ]\n",
      " [0.5435706 ]\n",
      " [0.599164  ]\n",
      " [0.4927046 ]\n",
      " [0.5531179 ]\n",
      " [0.56874657]\n",
      " [0.5067635 ]\n",
      " [0.56504965]\n",
      " [0.5541978 ]\n",
      " [0.59911114]\n",
      " [0.4926737 ]\n",
      " [0.5732316 ]\n",
      " [0.59688765]\n",
      " [0.5013998 ]\n",
      " [0.5526465 ]\n",
      " [0.5435646 ]\n",
      " [0.54255474]\n",
      " [0.45894307]\n",
      " [0.51607066]\n",
      " [0.55038595]\n",
      " [0.49893686]\n",
      " [0.44916606]\n",
      " [0.53094167]\n",
      " [0.52430284]\n",
      " [0.5657432 ]\n",
      " [0.5175146 ]\n",
      " [0.51566833]\n",
      " [0.50672406]\n",
      " [0.47341964]\n",
      " [0.56114024]\n",
      " [0.5902132 ]\n",
      " [0.6312173 ]\n",
      " [0.47160462]\n",
      " [0.5241099 ]\n",
      " [0.55235046]\n",
      " [0.54155797]\n",
      " [0.54312325]\n",
      " [0.57640946]\n",
      " [0.54371995]\n",
      " [0.56964993]\n",
      " [0.60849756]\n",
      " [0.55956894]\n",
      " [0.5165511 ]\n",
      " [0.55705345]\n",
      " [0.55455005]\n",
      " [0.5670475 ]\n",
      " [0.56277543]\n",
      " [0.4526386 ]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('./Kerastuner/untitled_project/predictions_output.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(predictions)    # Write the data rows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
