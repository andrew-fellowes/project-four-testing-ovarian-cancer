{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>SampleID</th>\n",
       "      <th>Source</th>\n",
       "      <th>MonthsOld</th>\n",
       "      <th>Purity</th>\n",
       "      <th>SeqRunID</th>\n",
       "      <th>DDMSampleID</th>\n",
       "      <th>MIDS</th>\n",
       "      <th>TotalReads(M)</th>\n",
       "      <th>lpWGSReads(M)</th>\n",
       "      <th>...</th>\n",
       "      <th>ResNoise</th>\n",
       "      <th>SignalNoiseRatio</th>\n",
       "      <th>QAStatus</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variant</th>\n",
       "      <th>%VariantFraction</th>\n",
       "      <th>MyriadGIScore</th>\n",
       "      <th>MyriadGIStatus</th>\n",
       "      <th>SOPHiAGIIndex</th>\n",
       "      <th>SophiaGIStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12749097</td>\n",
       "      <td>AZ</td>\n",
       "      <td>47</td>\n",
       "      <td>20</td>\n",
       "      <td>220121_NB501056_0748_AH2CV5BGXK</td>\n",
       "      <td>200058320-107-S2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.3</td>\n",
       "      <td>5.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.95</td>\n",
       "      <td>Medium</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12749205</td>\n",
       "      <td>AZ</td>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>220121_NB501056_0748_AH2CV5BGXK</td>\n",
       "      <td>200058326-106-S3</td>\n",
       "      <td>3</td>\n",
       "      <td>7.3</td>\n",
       "      <td>5.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.91</td>\n",
       "      <td>High</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>-15.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>12749267</td>\n",
       "      <td>AZ</td>\n",
       "      <td>71</td>\n",
       "      <td>20</td>\n",
       "      <td>220121_NB501056_0748_AH2CV5BGXK</td>\n",
       "      <td>200058327-104-S4</td>\n",
       "      <td>4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.64</td>\n",
       "      <td>High</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12749335</td>\n",
       "      <td>AZ</td>\n",
       "      <td>167</td>\n",
       "      <td>20</td>\n",
       "      <td>220121_NB501056_0748_AH2CV5BGXK</td>\n",
       "      <td>200058329-109-S6</td>\n",
       "      <td>6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.49</td>\n",
       "      <td>High</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>12749366</td>\n",
       "      <td>AZ</td>\n",
       "      <td>131</td>\n",
       "      <td>60</td>\n",
       "      <td>220121_NB501056_0748_AH2CV5BGXK</td>\n",
       "      <td>200058330-91-S7</td>\n",
       "      <td>7</td>\n",
       "      <td>8.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.18</td>\n",
       "      <td>High</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Run  SampleID Source MonthsOld Purity                         SeqRunID  \\\n",
       "0    1  12749097     AZ        47     20  220121_NB501056_0748_AH2CV5BGXK   \n",
       "1    1  12749205     AZ        93     30  220121_NB501056_0748_AH2CV5BGXK   \n",
       "2    1  12749267     AZ        71     20  220121_NB501056_0748_AH2CV5BGXK   \n",
       "3    1  12749335     AZ       167     20  220121_NB501056_0748_AH2CV5BGXK   \n",
       "4    1  12749366     AZ       131     60  220121_NB501056_0748_AH2CV5BGXK   \n",
       "\n",
       "        DDMSampleID  MIDS  TotalReads(M)  lpWGSReads(M)  ...  ResNoise  \\\n",
       "0  200058320-107-S2     2            7.3            5.9  ...      0.13   \n",
       "1  200058326-106-S3     3            7.3            5.6  ...      0.11   \n",
       "2  200058327-104-S4     4            9.6            6.1  ...       0.1   \n",
       "3  200058329-109-S6     6            8.9            5.6  ...      0.09   \n",
       "4   200058330-91-S7     7            8.6            5.0  ...      0.11   \n",
       "\n",
       "  SignalNoiseRatio QAStatus Gene Variant %VariantFraction MyriadGIScore  \\\n",
       "0             2.95   Medium    .       .                .            51   \n",
       "1             2.91     High    .       .                .            20   \n",
       "2             1.64     High    .       .                .            17   \n",
       "3             3.49     High    .       .                .            29   \n",
       "4             2.18     High    .       .                .            29   \n",
       "\n",
       "  MyriadGIStatus SOPHiAGIIndex SophiaGIStatus  \n",
       "0              1           3.2              1  \n",
       "1              2         -15.7              2  \n",
       "2              2          -4.6              2  \n",
       "3              2          -4.6              2  \n",
       "4              2          -8.2              2  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "# Provide the correct file path for charity_data.csv\n",
    "data_file_path = Path('./Resources/PeterMac_HRD_Validation.csv')\n",
    "data_df = pd.read_csv(data_file_path)\n",
    "data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>Source</th>\n",
       "      <th>MonthsOld</th>\n",
       "      <th>Purity</th>\n",
       "      <th>MIDS</th>\n",
       "      <th>TotalReads(M)</th>\n",
       "      <th>lpWGSReads(M)</th>\n",
       "      <th>TargetPanelReads(M)</th>\n",
       "      <th>%ReadslpWGS</th>\n",
       "      <th>%ReadsPanel</th>\n",
       "      <th>...</th>\n",
       "      <th>ResNoise</th>\n",
       "      <th>SignalNoiseRatio</th>\n",
       "      <th>QAStatus</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variant</th>\n",
       "      <th>%VariantFraction</th>\n",
       "      <th>MyriadGIScore</th>\n",
       "      <th>MyriadGIStatus</th>\n",
       "      <th>SOPHiAGIIndex</th>\n",
       "      <th>SophiaGIStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AZ</td>\n",
       "      <td>47</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>7.3</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>81%</td>\n",
       "      <td>19%</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.95</td>\n",
       "      <td>Medium</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AZ</td>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>7.3</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>76%</td>\n",
       "      <td>24%</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.91</td>\n",
       "      <td>High</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>-15.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>AZ</td>\n",
       "      <td>71</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>64%</td>\n",
       "      <td>36%</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.64</td>\n",
       "      <td>High</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AZ</td>\n",
       "      <td>167</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>63%</td>\n",
       "      <td>37%</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.49</td>\n",
       "      <td>High</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AZ</td>\n",
       "      <td>131</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>8.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>58%</td>\n",
       "      <td>42%</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.18</td>\n",
       "      <td>High</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Run Source MonthsOld Purity  MIDS  TotalReads(M)  lpWGSReads(M)  \\\n",
       "0    1     AZ        47     20     2            7.3            5.9   \n",
       "1    1     AZ        93     30     3            7.3            5.6   \n",
       "2    1     AZ        71     20     4            9.6            6.1   \n",
       "3    1     AZ       167     20     6            8.9            5.6   \n",
       "4    1     AZ       131     60     7            8.6            5.0   \n",
       "\n",
       "   TargetPanelReads(M) %ReadslpWGS %ReadsPanel  ... ResNoise SignalNoiseRatio  \\\n",
       "0                  1.4         81%         19%  ...     0.13             2.95   \n",
       "1                  1.7         76%         24%  ...     0.11             2.91   \n",
       "2                  3.5         64%         36%  ...      0.1             1.64   \n",
       "3                  3.3         63%         37%  ...     0.09             3.49   \n",
       "4                  3.6         58%         42%  ...     0.11             2.18   \n",
       "\n",
       "  QAStatus Gene Variant %VariantFraction MyriadGIScore  MyriadGIStatus  \\\n",
       "0   Medium    .       .                .            51               1   \n",
       "1     High    .       .                .            20               2   \n",
       "2     High    .       .                .            17               2   \n",
       "3     High    .       .                .            29               2   \n",
       "4     High    .       .                .            29               2   \n",
       "\n",
       "  SOPHiAGIIndex SophiaGIStatus  \n",
       "0           3.2              1  \n",
       "1         -15.7              2  \n",
       "2          -4.6              2  \n",
       "3          -4.6              2  \n",
       "4          -8.2              2  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the non-beneficial ID columns, SampleID, SeqRunID, DDMSampleID\n",
    "# Make these columns into One hot columns QAStatus, Gene, Variant\n",
    "# Make a Y value column we actually want to test \n",
    "# testing if there is a difference between MyriadGIStatus and SophiaGIStatus\n",
    "# Non_agreement = ABS(SophiaGIStatus - MyriadGIStatus)\n",
    "# Remove columns for X sample MyriadGIStatus, SophiaGIStatus, MyriadGIScore, SophiaGIIndex\n",
    "# Non_agreement\n",
    "# the Y value will be Non_agreement\n",
    "# We have to decide how to handle columns with actual non-values\n",
    "# being PurityPloidyRatio, Variant. Our lack of records and sparse grouping \n",
    "# makes these columns liklely to be dropped.\n",
    "#  YOUR CODE GOES HERE\n",
    "columns_to_drop = [\"SampleID\", \"SeqRunID\", \"DDMSampleID\"]\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "data_df = data_df.drop(columns=columns_to_drop, axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run                  numeric- 13 unique value(s)\n",
      "Source                                   (Data Type: object) - 4 unique value(s)\n",
      "MonthsOld                                (Data Type: object) - 78 unique value(s)\n",
      "Purity                                   (Data Type: object) - 14 unique value(s)\n",
      "MIDS                 numeric- 24 unique value(s)\n",
      "TotalReads(M)        numeric- 95 unique value(s)\n",
      "lpWGSReads(M)        numeric- 86 unique value(s)\n",
      "TargetPanelReads(M)  numeric- 64 unique value(s)\n",
      "%ReadslpWGS                              (Data Type: object) - 36 unique value(s)\n",
      "%ReadsPanel                              (Data Type: object) - 36 unique value(s)\n",
      "1000x                                    (Data Type: object) - 65 unique value(s)\n",
      "500x                                     (Data Type: object) - 56 unique value(s)\n",
      "200x                                     (Data Type: object) - 32 unique value(s)\n",
      "100x                                     (Data Type: object) - 16 unique value(s)\n",
      "50x                                      (Data Type: object) - 10 unique value(s)\n",
      "25x                                      (Data Type: object) - 6 unique value(s)\n",
      "DupFrac                                  (Data Type: object) - 36 unique value(s)\n",
      "LowCovRegions        numeric- 51 unique value(s)\n",
      "PurityPloidyRatio                        (Data Type: object) - 20 unique value(s)\n",
      "ResNoise                                 (Data Type: object) - 23 unique value(s)\n",
      "SignalNoiseRatio                         (Data Type: object) - 113 unique value(s)\n",
      "QAStatus                                 (Data Type: object) - 3 unique value(s)\n",
      "Gene                                     (Data Type: object) - 4 unique value(s)\n",
      "Variant                                  (Data Type: object) - 39 unique value(s)\n",
      "%VariantFraction                         (Data Type: object) - 39 unique value(s)\n",
      "MyriadGIScore        numeric- 70 unique value(s)\n",
      "MyriadGIStatus       numeric- 2 unique value(s)\n",
      "SOPHiAGIIndex                            (Data Type: object) - 105 unique value(s)\n",
      "SophiaGIStatus       numeric- 4 unique value(s)\n"
     ]
    }
   ],
   "source": [
    "# Finding attribute columns\n",
    "application_categories = data_df.dtypes[data_df.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "# Check the number of unique values in each column of object type\n",
    "columns = data_df[application_categories].nunique()\n",
    "\n",
    "# Iterate through the columns and print the unique value counts for each column\n",
    "# we iterate through every column in the dataframe, some of them of object type where we found the unique count\n",
    "for column in data_df.columns:\n",
    "    if column in columns.index:\n",
    "        data_type = data_df[column].dtype\n",
    "        print(f\"{column.ljust(40)} (Data Type: {data_type}) - {columns[column]} unique value(s)\")\n",
    "    else:\n",
    "        print(f\"{column.ljust(20)} numeric- {data_df[column].nunique()} unique value(s)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonthsOld    object\n",
      "Purity       object\n",
      "dtype: object\n",
      "MonthsOld    float64\n",
      "Purity       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert columns we will not bin, from pobject type to numeric\n",
    "column_names_to_convert = ['MonthsOld', 'Purity']\n",
    "\n",
    "# Step 1: Check the current data types of the columns\n",
    "print(data_df[column_names_to_convert].dtypes)\n",
    "\n",
    "# Step 2: Convert each column to numeric (if possible)\n",
    "for col in column_names_to_convert:\n",
    "    data_df[col] = pd.to_numeric(data_df[col], errors='coerce')\n",
    "\n",
    "# Step 3: Check the new data types of the columns after the conversion\n",
    "print(data_df[column_names_to_convert].dtypes)\n",
    "# print(data_df[\"Source\"].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['81%' '76%' '64%' '63%' '58%' '54%' '90%' '89%' '84%' '85%' '98%' '72%'\n",
      " '68%' '69%' '74%' '66%' '73%' '70%' '80%' '86%' '15%' '91%' '62%' '79%'\n",
      " '88%' '92%' '71%' '77%' '59%' '60%' '55%' '65%' '67%' '57%' '61%' '75%']\n",
      "['19%' '24%' '36%' '37%' '42%' '46%' '10%' '12%' '16%' '15%' '2%' '28%'\n",
      " '32%' '31%' '26%' '29%' '34%' '25%' '30%' '20%' '14%' '85%' '9%' '38%'\n",
      " '21%' '11%' '8%' '23%' '41%' '40%' '27%' '45%' '35%' '33%' '43%' '39%']\n"
     ]
    }
   ],
   "source": [
    "print(data_df['%ReadslpWGS'].unique())\n",
    "print(data_df['%ReadsPanel'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "# Create the Y column, Non_agreement = ABS(SophiaGIStatus - MyriadGIStatus)\n",
    "# data_df['Non_agreement'] = 1 - (abs(data_df['SophiaGIStatus'] - data_df['MyriadGIStatus']))\n",
    "data_df['Non_agreement'] = abs(data_df['SophiaGIStatus'] - data_df['MyriadGIStatus'])\n",
    "data_df['PurityPloidyRatio'] = data_df['PurityPloidyRatio'].replace('-', 0.0)\n",
    "data_df['ResNoise'] = data_df['ResNoise'].replace('-', 0.0)\n",
    "data_df['SignalNoiseRatio'] = data_df['SignalNoiseRatio'].replace('-', 0.0)\n",
    "\n",
    "data_df['Gene'] = data_df['Gene'].replace('.', 'Unlisted')\n",
    "\n",
    "\n",
    "data_df['MonthsOld'] = data_df['MonthsOld'].fillna(0.0)\n",
    "# data_df['MonthsOld'] = data_df['MonthsOld'].replace('.', 0.0)\n",
    "data_df['Purity'] = data_df['Purity'].replace('.', 0.0)\n",
    "data_df['%VariantFraction'] = data_df['%VariantFraction'].replace('.', 0.0)\n",
    "\n",
    "# Convert the 'Purity' column to numeric, replacing '.' with 0.0\n",
    "data_df['Purity'] = pd.to_numeric(data_df['Purity'], errors='coerce').fillna(0.0)\n",
    "\n",
    "data_df['DupFrac'] = data_df['DupFrac'].replace('%', '', regex=True).astype(float)\n",
    "data_df['%ReadslpWGS'] = data_df['%ReadslpWGS'].replace('%', '', regex=True).astype(float)\n",
    "data_df['%ReadsPanel'] = data_df['%ReadsPanel'].replace('%', '', regex=True).astype(float)\n",
    "\n",
    "data_df['Variant'] = data_df['Variant'].replace('.', 'Unlisted')\n",
    "\n",
    "# Apply label encoding to 'Variant' column\n",
    "label_encoder = LabelEncoder()\n",
    "data_df['Variant'] = label_encoder.fit_transform(data_df['Variant'].astype(str))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_df['1000x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "\n",
    "data_df['500x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "data_df['200x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "data_df['100x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "data_df['50x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "data_df['25x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "\n",
    "print(data_df['Non_agreement'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  QAStatus  COUNT\n",
      "0     High     89\n",
      "1   Medium     42\n",
      "2      Low      8\n",
      "---\n",
      "       Gene  COUNT\n",
      "0  Unlisted     99\n",
      "1     BRCA1     21\n",
      "2     BRCA2     18\n",
      "3    RAD51D      1\n",
      "---\n",
      "    Variant  COUNT\n",
      "0         0     99\n",
      "1        14      2\n",
      "2        21      2\n",
      "3        36      1\n",
      "4        27      1\n",
      "5        22      1\n",
      "6        23      1\n",
      "7        24      1\n",
      "8        25      1\n",
      "9        37      1\n",
      "10       26      1\n",
      "11       28      1\n",
      "12       35      1\n",
      "13       20      1\n",
      "14       30      1\n",
      "15       31      1\n",
      "16       32      1\n",
      "17       33      1\n",
      "18       34      1\n",
      "19       29      1\n",
      "20       19      1\n",
      "21        1      1\n",
      "22       18      1\n",
      "23        2      1\n",
      "24        3      1\n",
      "25        4      1\n",
      "26        5      1\n",
      "27        6      1\n",
      "28        7      1\n",
      "29        8      1\n",
      "30        9      1\n",
      "31       10      1\n",
      "32       11      1\n",
      "33       12      1\n",
      "34       13      1\n",
      "35       15      1\n",
      "36       16      1\n",
      "37       17      1\n",
      "38       38      1\n",
      "---\n",
      "   PurityPloidyRatio  COUNT\n",
      "0                0.0     46\n",
      "1               0.23     15\n",
      "2               0.25     11\n",
      "3               0.45      9\n",
      "4                0.3      8\n",
      "5               0.28      7\n",
      "6               0.33      6\n",
      "7                0.2      5\n",
      "8               0.35      5\n",
      "9               0.38      5\n",
      "10              0.15      4\n",
      "11                 0      3\n",
      "12              0.42      3\n",
      "13              0.47      3\n",
      "14              0.17      2\n",
      "15              0.12      2\n",
      "16               0.4      2\n",
      "17               0.1      1\n",
      "18              0.07      1\n",
      "19               0.5      1\n"
     ]
    }
   ],
   "source": [
    "# Look at APPLICATION_TYPE value counts for binning  QAStatus\n",
    "\n",
    "grouped_df = data_df.groupby(\"QAStatus\").size().reset_index(name=\"COUNT\")\n",
    "sorted_df = grouped_df.sort_values(by=\"COUNT\", ascending=False)\n",
    "sorted_df = sorted_df.reset_index(drop=True)\n",
    "print(sorted_df)\n",
    "print('---')\n",
    "grouped_df = data_df.groupby(\"Gene\").size().reset_index(name=\"COUNT\")\n",
    "sorted_df = grouped_df.sort_values(by=\"COUNT\", ascending=False)\n",
    "sorted_df = sorted_df.reset_index(drop=True)\n",
    "print(sorted_df)\n",
    "print('---')\n",
    "grouped_df = data_df.groupby(\"Variant\").size().reset_index(name=\"COUNT\")\n",
    "sorted_df = grouped_df.sort_values(by=\"COUNT\", ascending=False)\n",
    "sorted_df = sorted_df.reset_index(drop=True)\n",
    "print(sorted_df)\n",
    "print('---')\n",
    "grouped_df = data_df.groupby(\"PurityPloidyRatio\").size().reset_index(name=\"COUNT\")\n",
    "sorted_df = grouped_df.sort_values(by=\"COUNT\", ascending=False)\n",
    "sorted_df = sorted_df.reset_index(drop=True)\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dummy block of code in case we want to gather many column values into the same bucket\n",
    "# application_types_to_replace = [\"T9\", \"T13\", \"T12\", \"T2\", \"T14\", \"T25\", \"T29\", \"T15\", \"T17\"]\n",
    "\n",
    "# Replace the specified values in the \"APPLICATION_TYPE\" column with \"Other\"\n",
    "# data_df['APPLICATION_TYPE'] = data_df['APPLICATION_TYPE'].replace(application_types_to_replace, \"Other\")\n",
    "\n",
    "# Check the value counts after replacing\n",
    "# print(data_df['APPLICATION_TYPE'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run                      int64\n",
      "Source                  object\n",
      "MonthsOld              float64\n",
      "Purity                 float64\n",
      "MIDS                     int64\n",
      "TotalReads(M)          float64\n",
      "lpWGSReads(M)          float64\n",
      "TargetPanelReads(M)    float64\n",
      "%ReadslpWGS            float64\n",
      "%ReadsPanel            float64\n",
      "1000x                  float64\n",
      "500x                   float64\n",
      "200x                   float64\n",
      "100x                   float64\n",
      "50x                    float64\n",
      "25x                    float64\n",
      "DupFrac                float64\n",
      "LowCovRegions            int64\n",
      "PurityPloidyRatio       object\n",
      "ResNoise                object\n",
      "SignalNoiseRatio        object\n",
      "QAStatus                object\n",
      "Gene                    object\n",
      "Variant                  int32\n",
      "%VariantFraction        object\n",
      "MyriadGIScore            int64\n",
      "MyriadGIStatus           int64\n",
      "SOPHiAGIIndex           object\n",
      "SophiaGIStatus           int64\n",
      "Non_agreement            int64\n",
      "Variant                  int32\n",
      "QAStatus_High            uint8\n",
      "QAStatus_Low             uint8\n",
      "QAStatus_Medium          uint8\n",
      "Gene_BRCA1               uint8\n",
      "Gene_BRCA2               uint8\n",
      "Gene_RAD51D              uint8\n",
      "Gene_Unlisted            uint8\n",
      "Source_AZ                uint8\n",
      "Source_BRAZIL            uint8\n",
      "Source_GREECE            uint8\n",
      "Source_WEHI              uint8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create one-hot columns for these 3 columns\n",
    "onehot_cols = [\"QAStatus\", \"Gene\", \"Variant\", \"Source\",]  #\"PurityPloidyRatio\"]\n",
    "\n",
    "# Use get_dummies() to one-hot encode only the categorical columns\n",
    "one_hot_encoded = pd.get_dummies(data_df[onehot_cols])\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "data_df = pd.concat([data_df, one_hot_encoded], axis=1)\n",
    "\n",
    "# After this, you can print the data types of columns in the 'data_df' DataFrame\n",
    "column_types = data_df.dtypes\n",
    "print(column_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_callback(message):\n",
    "    print(f\"Final callback: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_accuracy_callback(epoch, loss, accuracy):\n",
    "    print(f\"Epoch {epoch}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from datetime import datetime\n",
    "\n",
    "def main_process(callback, X_train_scaled, y_train, nn, report_interval=5):\n",
    "    # Train the model and store the training history\n",
    "    fit_model = nn.fit(X_train_scaled, y_train, epochs=100, verbose=0, callbacks=[callback])  # Pass the callback here\n",
    "    training_history = fit_model.history\n",
    "    \n",
    "    print(\"Training has started.\")\n",
    "    epochs = 100\n",
    "    \n",
    "    # Report loss and accuracy at the specified intervals\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        if epoch % report_interval == 0 or epoch == epochs:\n",
    "            loss = training_history['loss'][epoch - 1]\n",
    "            accuracy = training_history['accuracy'][epoch - 1]\n",
    "            print(f\"Epoch {epoch}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\")#soos2\n",
    "            callback.on_epoch_end(epoch, {'loss': loss, 'accuracy': accuracy})  # Manually call the on_epoch_end method\n",
    "\n",
    "    result = \"Task completed.\"\n",
    "    # final_callback(result)  # Comment out or remove this line as it is not defined in the code\n",
    "    print(\"Main process finished.\")\n",
    "\n",
    "# soos\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "script_name = \"Starter_Codev1\"\n",
    "csv_loss_accuracy_file_name = f\"{script_name}_test_result_{current_datetime}.csv\"\n",
    "\n",
    "# Create the CSVLogger callback and pass the file name\n",
    "csv_logger = CSVLogger(csv_loss_accuracy_file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "### In our original model, we created data frame application_df, which still exists.\n",
    "- 1. I will create a pca method on this data frame which has had bucketing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Run', 'MonthsOld', 'Purity', 'MIDS', 'TotalReads(M)', 'lpWGSReads(M)',\n",
       "       'TargetPanelReads(M)', '%ReadslpWGS', '%ReadsPanel', '1000x', '500x',\n",
       "       '200x', '100x', '50x', '25x', 'DupFrac', 'LowCovRegions',\n",
       "       'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio', 'Variant',\n",
       "       '%VariantFraction', 'Variant', 'QAStatus_High', 'QAStatus_Low',\n",
       "       'QAStatus_Medium', 'Gene_BRCA1', 'Gene_BRCA2', 'Gene_RAD51D',\n",
       "       'Gene_Unlisted', 'Source_AZ', 'Source_BRAZIL', 'Source_GREECE',\n",
       "       'Source_WEHI'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1  pca method on application_df which has had bucketing performed for the original model.\n",
    "data_df_x = data_df.copy()\n",
    "data_df_x = data_df_x.drop(columns=[\"Gene\",\"Source\",\"Non_agreement\", \"MyriadGIStatus\", \"SophiaGIStatus\", \"MyriadGIScore\", \"SOPHiAGIIndex\",\"QAStatus\"], axis=1)\n",
    "data_df_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: bool)\n"
     ]
    }
   ],
   "source": [
    "#data_df_x\n",
    "# Step 1: Check which columns have missing values (NaN)\n",
    "columns_with_missing_values = data_df_x.isna().any()\n",
    "\n",
    "# Step 2: Output the columns where the count of missing values is larger than 0\n",
    "columns_with_missing_values = columns_with_missing_values[columns_with_missing_values]\n",
    "\n",
    "# Output the columns with missing values\n",
    "print(columns_with_missing_values)\n",
    "#print(data_df_x['%ReadslpWGS'].unique())\n",
    "#print(data_df_x['%ReadsPanel'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0 '42.6' '78.1' '78.5' '77.2' '22.1' '37.8' '56.4' '24.4' '87' '54.6'\n",
      " '51.1' '85.8' '71.8' '66.1' '9' '87.4' '84.1' '80.4' '73' '67.6' '17.3'\n",
      " '9.1' '35.8' '82.8' '80.5' 'Deleted' '18.7' '65.3' '51.8' '68.8' '62.6'\n",
      " '71.2' '89.8' '47' '59.6' '5.4' '26.7' '64.7']\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the \"%VariantFraction\" column\n",
    "print(data_df['%VariantFraction'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Deleted\" with 0 in the '%ReadslpWGS' column\n",
    "data_df_x['%VariantFraction'] = data_df_x['%VariantFraction'].replace('Deleted', 0)\n",
    "\n",
    "# Convert the column to numeric (float) format\n",
    "data_df_x['%VariantFraction'] = pd.to_numeric(data_df_x['%VariantFraction'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns:\n",
      "Index(['PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#data_df_x.columns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data_df_x' is the DataFrame containing the data\n",
    "non_numeric_columns = data_df_x.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "print(\"Non-numeric columns:\")\n",
    "print(non_numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "non_numeric_columns = data_df_x.select_dtypes(exclude=[np.number]).columns\n",
    "print(non_numeric_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run                      int64\n",
      "MonthsOld              float64\n",
      "Purity                 float64\n",
      "MIDS                     int64\n",
      "TotalReads(M)          float64\n",
      "lpWGSReads(M)          float64\n",
      "TargetPanelReads(M)    float64\n",
      "%ReadslpWGS            float64\n",
      "%ReadsPanel            float64\n",
      "1000x                  float64\n",
      "500x                   float64\n",
      "200x                   float64\n",
      "100x                   float64\n",
      "50x                    float64\n",
      "25x                    float64\n",
      "DupFrac                float64\n",
      "LowCovRegions            int64\n",
      "PurityPloidyRatio       object\n",
      "ResNoise                object\n",
      "SignalNoiseRatio        object\n",
      "Variant                  int32\n",
      "%VariantFraction       float64\n",
      "Variant                  int32\n",
      "QAStatus_High            uint8\n",
      "QAStatus_Low             uint8\n",
      "QAStatus_Medium          uint8\n",
      "Gene_BRCA1               uint8\n",
      "Gene_BRCA2               uint8\n",
      "Gene_RAD51D              uint8\n",
      "Gene_Unlisted            uint8\n",
      "Source_AZ                uint8\n",
      "Source_BRAZIL            uint8\n",
      "Source_GREECE            uint8\n",
      "Source_WEHI              uint8\n",
      "dtype: object\n",
      "Run                    0\n",
      "MonthsOld              0\n",
      "Purity                 0\n",
      "MIDS                   0\n",
      "TotalReads(M)          0\n",
      "lpWGSReads(M)          0\n",
      "TargetPanelReads(M)    0\n",
      "%ReadslpWGS            0\n",
      "%ReadsPanel            0\n",
      "1000x                  0\n",
      "500x                   0\n",
      "200x                   0\n",
      "100x                   0\n",
      "50x                    0\n",
      "25x                    0\n",
      "DupFrac                0\n",
      "LowCovRegions          0\n",
      "PurityPloidyRatio      0\n",
      "ResNoise               0\n",
      "SignalNoiseRatio       0\n",
      "Variant                0\n",
      "%VariantFraction       0\n",
      "Variant                0\n",
      "QAStatus_High          0\n",
      "QAStatus_Low           0\n",
      "QAStatus_Medium        0\n",
      "Gene_BRCA1             0\n",
      "Gene_BRCA2             0\n",
      "Gene_RAD51D            0\n",
      "Gene_Unlisted          0\n",
      "Source_AZ              0\n",
      "Source_BRAZIL          0\n",
      "Source_GREECE          0\n",
      "Source_WEHI            0\n",
      "dtype: int64\n",
      "Run                    0\n",
      "MonthsOld              0\n",
      "Purity                 0\n",
      "MIDS                   0\n",
      "TotalReads(M)          0\n",
      "lpWGSReads(M)          0\n",
      "TargetPanelReads(M)    0\n",
      "%ReadslpWGS            0\n",
      "%ReadsPanel            0\n",
      "1000x                  0\n",
      "500x                   0\n",
      "200x                   0\n",
      "100x                   0\n",
      "50x                    0\n",
      "25x                    0\n",
      "DupFrac                0\n",
      "LowCovRegions          0\n",
      "PurityPloidyRatio      0\n",
      "ResNoise               0\n",
      "SignalNoiseRatio       0\n",
      "Variant                0\n",
      "%VariantFraction       0\n",
      "Variant                0\n",
      "QAStatus_High          0\n",
      "QAStatus_Low           0\n",
      "QAStatus_Medium        0\n",
      "Gene_BRCA1             0\n",
      "Gene_BRCA2             0\n",
      "Gene_RAD51D            0\n",
      "Gene_Unlisted          0\n",
      "Source_AZ              0\n",
      "Source_BRAZIL          0\n",
      "Source_GREECE          0\n",
      "Source_WEHI            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check data types of the columns in data_df_x\n",
    "print(data_df_x.dtypes)\n",
    "\n",
    "# Check for missing values in data_df_x\n",
    "print(data_df_x.isnull().sum())\n",
    "\n",
    "# Convert the '%VariantFraction' column to numeric values, invalid values will be converted to NaN\n",
    "data_df_x['%VariantFraction'] = pd.to_numeric(data_df_x['%VariantFraction'], errors='coerce')\n",
    "\n",
    "# Create a mask to identify rows with NaN values in the '%VariantFraction' column\n",
    "invalid_rows_mask = data_df_x['%VariantFraction'].isna()\n",
    "\n",
    "# Use the mask to filter the DataFrame and get the rows with invalid values\n",
    "invalid_rows = data_df_x[invalid_rows_mask]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if any non-numeric values still exist in data_df_x\n",
    "non_numeric_values = data_df_x.apply(pd.to_numeric, errors='coerce').isnull().sum()\n",
    "print(non_numeric_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: Index(['PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio'], dtype='object')\n",
      "Missing values:\n",
      " Run                    0\n",
      "MonthsOld              0\n",
      "Purity                 0\n",
      "MIDS                   0\n",
      "TotalReads(M)          0\n",
      "lpWGSReads(M)          0\n",
      "TargetPanelReads(M)    0\n",
      "%ReadslpWGS            0\n",
      "%ReadsPanel            0\n",
      "1000x                  0\n",
      "500x                   0\n",
      "200x                   0\n",
      "100x                   0\n",
      "50x                    0\n",
      "25x                    0\n",
      "DupFrac                0\n",
      "LowCovRegions          0\n",
      "PurityPloidyRatio      0\n",
      "ResNoise               0\n",
      "SignalNoiseRatio       0\n",
      "Variant                0\n",
      "%VariantFraction       0\n",
      "Variant                0\n",
      "QAStatus_High          0\n",
      "QAStatus_Low           0\n",
      "QAStatus_Medium        0\n",
      "Gene_BRCA1             0\n",
      "Gene_BRCA2             0\n",
      "Gene_RAD51D            0\n",
      "Gene_Unlisted          0\n",
      "Source_AZ              0\n",
      "Source_BRAZIL          0\n",
      "Source_GREECE          0\n",
      "Source_WEHI            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for non-numeric columns\n",
    "non_numeric_cols = data_df_x.select_dtypes(exclude=[np.number]).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data_df_x.isnull().sum()\n",
    "print(\"Missing values:\\n\", missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Run', 'MonthsOld', 'Purity', 'MIDS', 'TotalReads(M)', 'lpWGSReads(M)',\n",
       "       'TargetPanelReads(M)', '%ReadslpWGS', '%ReadsPanel', '1000x', '500x',\n",
       "       '200x', '100x', '50x', '25x', 'DupFrac', 'LowCovRegions',\n",
       "       'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio', '%VariantFraction',\n",
       "       'QAStatus_High', 'QAStatus_Low', 'QAStatus_Medium', 'Gene_BRCA1',\n",
       "       'Gene_BRCA2', 'Gene_RAD51D', 'Gene_Unlisted', 'Source_AZ',\n",
       "       'Source_BRAZIL', 'Source_GREECE', 'Source_WEHI'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the duplicate 'Variant' column\n",
    "data_df_x = data_df_x.drop(columns='Variant')\n",
    "\n",
    "data_df_x.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Run, MonthsOld, Purity, MIDS, TotalReads(M), lpWGSReads(M), TargetPanelReads(M), %ReadslpWGS, %ReadsPanel, 1000x, 500x, 200x, 100x, 50x, 25x, DupFrac, LowCovRegions, PurityPloidyRatio, ResNoise, SignalNoiseRatio, %VariantFraction, QAStatus_High, QAStatus_Low, QAStatus_Medium, Gene_BRCA1, Gene_BRCA2, Gene_RAD51D, Gene_Unlisted, Source_AZ, Source_BRAZIL, Source_GREECE, Source_WEHI]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "invalid_rows = data_df_x['%VariantFraction'].apply(lambda x: not str(x).replace('.', '').isnumeric())\n",
    "\n",
    "# Display the rows containing the invalid values\n",
    "print(data_df_x[invalid_rows])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139, 32)\n",
      "(111, 32) (28, 32)\n",
      "(111, 3) (28, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume data_df_x contains the features, and data_df contains the target \"Non_agreement\" column\n",
    "\n",
    "# Split our preprocessed data into our features and target arrays\n",
    "y2 = data_df[\"Non_agreement\"].values\n",
    "X2 = data_df_x.values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=78)\n",
    "\n",
    "# Now perform PCA only on the training data\n",
    "pca = PCA(n_components=3)\n",
    "X2_train_pca = pca.fit_transform(X2_train)\n",
    "\n",
    "# Apply the same PCA transformation to the testing data\n",
    "X2_test_pca = pca.transform(X2_test)\n",
    "\n",
    "\n",
    "# Check the number of records in the original dataset\n",
    "print(data_df_x.shape)\n",
    "\n",
    "# Check the number of records after splitting into training and testing sets\n",
    "print(X2_train.shape, X2_test.shape)\n",
    "\n",
    "# Check the number of records after PCA transformation\n",
    "print(X2_train_pca.shape, X2_test_pca.shape)\n",
    "\n",
    "# Create a new DataFrame with the PCA data for both training and testing sets\n",
    "df_train_pca = pd.DataFrame(X2_train_pca, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "df_test_pca = pd.DataFrame(X2_test_pca, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X2_scaler = scaler.fit(X2_train)\n",
    "\n",
    "# Scale the data\n",
    "X2_train_scaled = X2_scaler.transform(X2_train)\n",
    "X2_test_scaled = X2_scaler.transform(X2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate my PCA bucketted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 6)                 198       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 56        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 305\n",
      "Trainable params: 305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "#  YOUR CODE GOES HERE\n",
    "import tensorflow as tf\n",
    "number_input_features = len(X2_train[0])\n",
    "hidden_nodes_layer1 = 6\n",
    "hidden_nodes_layer2 = 8\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1,\n",
    "input_dim=number_input_features, activation = \"relu\")\n",
    ")\n",
    "# I am setting the regularization\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"tanh\"))\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "#nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Check the structure of the model\n",
    "\n",
    "nn.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "#  YOUR CODE GOES HERE\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started.\n",
      "Epoch 5: Loss=0.6505, Accuracy=0.5315\n",
      "Epoch 10: Loss=0.5927, Accuracy=0.6847\n",
      "Epoch 15: Loss=0.5457, Accuracy=0.7207\n",
      "Epoch 20: Loss=0.5006, Accuracy=0.7568\n",
      "Epoch 25: Loss=0.4541, Accuracy=0.7568\n",
      "Epoch 30: Loss=0.4057, Accuracy=0.7748\n",
      "Epoch 35: Loss=0.3549, Accuracy=0.7838\n",
      "Epoch 40: Loss=0.3011, Accuracy=0.7748\n",
      "Epoch 45: Loss=0.2486, Accuracy=0.8018\n",
      "Epoch 50: Loss=0.2000, Accuracy=0.8018\n",
      "Epoch 55: Loss=0.1599, Accuracy=0.8108\n",
      "Epoch 60: Loss=0.1219, Accuracy=0.8198\n",
      "Epoch 65: Loss=0.0820, Accuracy=0.8198\n",
      "Epoch 70: Loss=0.0477, Accuracy=0.8108\n",
      "Epoch 75: Loss=0.0160, Accuracy=0.8018\n",
      "Epoch 80: Loss=-0.0136, Accuracy=0.8108\n",
      "Epoch 85: Loss=-0.0411, Accuracy=0.8108\n",
      "Epoch 90: Loss=-0.0673, Accuracy=0.8198\n",
      "Epoch 95: Loss=-0.0938, Accuracy=0.8288\n",
      "Epoch 100: Loss=-0.1191, Accuracy=0.8378\n",
      "Main process finished.\n"
     ]
    }
   ],
   "source": [
    "#main_process(loss_accuracy_callback, X2_train_scaled, y2_train, nn)\n",
    "# Start the training and use the CSVLogger callback to automatically log epoch, loss, and accuracy data to the CSV file\n",
    "main_process(csv_logger, X2_train_scaled, y2_train, nn, report_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'class_name': 'InputLayer', 'config': {'batch_input_shape': [None, 43], 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'dense_input'}}, {'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'batch_input_shape': [None, 43], 'dtype': 'float32', 'units': 8, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 5, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from datetime import datetime\n",
    "\n",
    "# ... Your code to create and train the neural network ...\n",
    "# ... Assuming you already have the `nn` model ...\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define the script name manually (replace 'Your_Script_Name' with the actual name of your script)\n",
    "script_name = \"PCAProject4_Codev1\"\n",
    "\n",
    "# Generate the output file name with the script name and date/time\n",
    "output_file_name = f\"{script_name}_{current_datetime}.h5\"\n",
    "\n",
    "# Save the model with the date and time in the file name\n",
    "nn.save(output_file_name)\n",
    "\n",
    "# Output CSV file name with the script name and date/time\n",
    "csv_file_name = f\"{script_name}_model_data_{current_datetime}.csv\"\n",
    "\n",
    "# Extract model configuration (architecture and hyperparameters)\n",
    "model_config = nn.get_config()\n",
    "\n",
    "# Write the model information to the CSV file\n",
    "with open(csv_file_name, \"w\", newline=\"\") as csv_file:\n",
    "    fieldnames = model_config.keys()  # Retrieve model configuration keys\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()  # Write header with field names\n",
    "    writer.writerow(model_config)  # Write the model configuration to the CSV\n",
    "\n",
    "# Corrected JSON-like string\n",
    "corrected_json_str = '[{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 43], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"batch_input_shape\": [null, 43], \"dtype\": \"float32\", \"units\": 8, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 5, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]'\n",
    "\n",
    "# Convert the JSON-like string to a list of dictionaries\n",
    "layers_list = json.loads(corrected_json_str)\n",
    "\n",
    "# Print the list of dictionaries\n",
    "print(layers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.2058 - accuracy: 0.6071 - 207ms/epoch - 207ms/step\n",
      "Loss: 0.20579825341701508, Accuracy: 0.6071428656578064\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def write_test_result_to_csv(file_name, loss, accuracy, current_datetime):\n",
    "    with open(file_name, \"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"Metric\", \"Value\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()  # Write header with field names\n",
    "        writer.writerow({\"Metric\": \"Loss\", \"Value\": loss})\n",
    "        writer.writerow({\"Metric\": \"Accuracy\", \"Value\": accuracy})\n",
    "        writer.writerow({\"Metric\": \"Date\", \"Value\": current_datetime})  # Write current date and time\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X2_test_scaled, y2_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "# Output CSV file name for test results with the script name and date/time\n",
    "test_result_csv_file = f\"{script_name}_loss_accuracy_{current_datetime}.csv\"\n",
    "\n",
    "# Write the test result to the CSV file with the current date and time\n",
    "write_test_result_to_csv(test_result_csv_file, model_loss, model_accuracy, current_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 150ms/step\n",
      "[[18  6]\n",
      " [ 4  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have a trained model named 'nn' and test data 'X2_test_scaled', 'y2_test'\n",
    "predictions = nn.predict(X2_test_scaled)\n",
    "\n",
    "# Convert the probability predictions to class labels\n",
    "predictions = (predictions > 0.5).astype(int)  # Assuming the threshold is 0.5 for binary classification\n",
    "\n",
    "# Choose the class for which you want to calculate the binary confusion matrix\n",
    "positive_class = 1\n",
    "\n",
    "# Create binary labels for the positive class\n",
    "y_true_binary = (y2_test == positive_class).astype(int)\n",
    "\n",
    "# Calculate the binary confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_binary, predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "print(conf_matrix)\n",
    "\n",
    "confusion_csv_file = f\"{script_name}_confusion_matrix_{current_datetime}.csv\"\n",
    "\n",
    "np.savetxt(confusion_csv_file, conf_matrix, delimiter=\",\", fmt='%.2f')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Run', 'MonthsOld', 'Purity', 'MIDS', 'TotalReads(M)', 'lpWGSReads(M)',\n",
       "       'TargetPanelReads(M)', '%ReadslpWGS', '%ReadsPanel', '1000x', '500x',\n",
       "       '200x', '100x', '50x', '25x', 'DupFrac', 'LowCovRegions',\n",
       "       'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio', '%VariantFraction',\n",
       "       'QAStatus_High', 'QAStatus_Low', 'QAStatus_Medium', 'Gene_BRCA1',\n",
       "       'Gene_BRCA2', 'Gene_RAD51D', 'Gene_Unlisted', 'Source_AZ',\n",
       "       'Source_BRAZIL', 'Source_GREECE', 'Source_WEHI'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'PurityPloidyRatio' does not exist in the test data. Setting default value to -1.\n",
      "Column 'ResNoise' does not exist in the test data. Setting default value to -1.\n",
      "Column 'SignalNoiseRatio' does not exist in the test data. Setting default value to -1.\n",
      "[(12,   4., 70.,  9, 17. , 11.6, 5.4, 68., 32., 67., 67., 67., 67., 67., 67., 66.,   0, 0.28, 0.14, 1.66, 0., 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0)\n",
      " ( 6,  26., 70., 11, 14.1, 10.3, 3.8, 73., 27.,  5.,  5.,  5.,  5.,  5.,  5., 81.,  34, 0.35, 0.23, 1.35, 0., 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0)\n",
      " (11,   0.,  0.,  6, 20.3, 14.1, 6.2, 70., 30., 96., 96., 96., 96., 96., 96., 62.,   0, 0.1 , 0.06, 2.14, 0., 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0)\n",
      " (11,   0.,  0., 18, 18.4, 14.9, 3.5, 81., 19., 12., 12., 12., 12., 12., 12., 65.,   3, 0.  , 0.07, 1.96, 0., 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0)\n",
      " ( 2, 135., 40.,  7, 16.6, 11.5, 5.1, 69., 31.,  1.,  1.,  1.,  1.,  1.,  1., 92., 526, 0.23, 0.08, 3.04, 0., 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "# Convert the non-numeric columns with 'NA' values to -1\n",
    "non_numeric_columns = ['PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio']\n",
    "for col in non_numeric_columns:\n",
    "    try:\n",
    "        column_idx = np.where(X2_test.dtype.names == col)[0][0]\n",
    "        X2_test[col] = np.where(X2_test[col] == 'NA', -1, X2_test[col])\n",
    "    except IndexError:\n",
    "        print(f\"Column '{col}' does not exist in the test data. Setting default value to -1.\")\n",
    "\n",
    "# Concatenate the processed X_test and y_test data\n",
    "data_to_save = np.column_stack((X2_test, y2_test))\n",
    "\n",
    "# Save the concatenated data to the file and append to the existing file\n",
    "with open('processed_test_data.csv', 'ab') as f:\n",
    "    np.savetxt(f, data_to_save, delimiter=',', fmt='%s')\n",
    "\n",
    "# Load the saved data from the file and print the first few rows\n",
    "saved_data = np.genfromtxt('processed_test_data.csv', delimiter=',', dtype=None, names=True, encoding=None)\n",
    "print(saved_data[:5])\n",
    "\n",
    "\n",
    "#confusion_csv_file = f\"{script_name}_processed_test_data_{current_datetime}.csv\"\n",
    "np.savetxt(confusion_csv_file, conf_matrix, delimiter=\",\", fmt='%.2f')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'PurityPloidyRatio' does not exist in the test data. Setting default value to -1.\n",
      "Column 'ResNoise' does not exist in the test data. Setting default value to -1.\n",
      "Column 'SignalNoiseRatio' does not exist in the test data. Setting default value to -1.\n",
      "[(12,   4., 70.,  9, 17. , 11.6, 5.4, 68., 32., 67., 67., 67., 67., 67., 67., 66.,   0, 0.28, 0.14, 1.66, 0., 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0)\n",
      " ( 6,  26., 70., 11, 14.1, 10.3, 3.8, 73., 27.,  5.,  5.,  5.,  5.,  5.,  5., 81.,  34, 0.35, 0.23, 1.35, 0., 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0)\n",
      " (11,   0.,  0.,  6, 20.3, 14.1, 6.2, 70., 30., 96., 96., 96., 96., 96., 96., 62.,   0, 0.1 , 0.06, 2.14, 0., 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0)\n",
      " (11,   0.,  0., 18, 18.4, 14.9, 3.5, 81., 19., 12., 12., 12., 12., 12., 12., 65.,   3, 0.  , 0.07, 1.96, 0., 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0)\n",
      " ( 2, 135., 40.,  7, 16.6, 11.5, 5.1, 69., 31.,  1.,  1.,  1.,  1.,  1.,  1., 92., 526, 0.23, 0.08, 3.04, 0., 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Convert the non-numeric columns with 'NA' values to -1\n",
    "non_numeric_columns = ['PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio']\n",
    "for col in non_numeric_columns:\n",
    "    try:\n",
    "        column_idx = np.where(X2_test.dtype.names == col)[0][0]\n",
    "        X2_test[col] = np.where(X2_test[col] == 'NA', -1, X2_test[col])\n",
    "    except IndexError:\n",
    "        print(f\"Column '{col}' does not exist in the test data. Setting default value to -1.\")\n",
    "\n",
    "# Concatenate the processed X_test and y_test data\n",
    "data_to_save = np.column_stack((X2_test, y2_test))\n",
    "\n",
    "# Create a filename with the current datetime\n",
    "#current_datetime = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "csv_file_name = f\"processed_test_data_{current_datetime}.csv\"\n",
    "\n",
    "# Save the concatenated data to the file\n",
    "np.savetxt(csv_file_name, data_to_save, delimiter=',', fmt='%s')\n",
    "\n",
    "# Load the saved data from the file and print the first few rows\n",
    "saved_data = np.genfromtxt(csv_file_name, delimiter=',', dtype=None, names=True, encoding=None)\n",
    "print(saved_data[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
