{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "# Provide the correct file path for our training file and our file to be predicted\n",
    "data_file_path = Path('./Resources/PeterMac_HRD_Validation.csv')\n",
    "predict_data_file_path = ('./Resources/correct_PeterMac_HRD_clinical_data.csv')\n",
    "\n",
    "data_df = pd.read_csv(data_file_path)\n",
    "data_df.head()\n",
    "\n",
    "data_df_predict = pd.read_csv(predict_data_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial ID columns, SampleID, SeqRunID, DDMSampleID\n",
    "# Make these columns into One hot columns QAStatus, Gene, Variant\n",
    "# Make a Y value column we actually want to test \n",
    "# testing if there is a difference between MyriadGIStatus and SophiaGIStatus\n",
    "# Non-agreement \n",
    "# As we are testing the agreement of MyriadGiStatus(1/2) and SophiaGIStatus(1/2/3/4), we need to \n",
    "# collapse the test into a binary result.\n",
    "#    MyriadGIStatus        SophiaGIStatus      Laboratory Classsification  Binary Result\n",
    "#       1                       1                   TruePositive                1\n",
    "#       1                       2                   False Negative              0\n",
    "#       1                       3                   Inconclusive                0\n",
    "#       1                       4                   Inconclusive                0\n",
    "#       2                       1                   False Positive              0\n",
    "#       2                       2                   True Negative               1\n",
    "#       2                       3                   Inconclusive                0\n",
    "#       2                       4                   Inconclusive                0\n",
    "# Remove columns for X sample MyriadGIStatus, SophiaGIStatus, MyriadGIScore, SophiaGIIndex\n",
    "# Non-agreement\n",
    "# the Y value will be Non-agreement\n",
    "# We have to decide how to handle columns with actual non-values\n",
    "# being PurityPloidyRatio, Variant. Our lack of records and sparse grouping \n",
    "# makes these columns liklely to be dropped.\n",
    "#  YOUR CODE GOES HERE\n",
    "columns_to_drop = [\"SampleID\", \"SeqRunID\", \"DDMSampleID\",\"MonthsOld\"]\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "data_df = data_df.drop(columns=columns_to_drop, axis=1)\n",
    "data_df.head()\n",
    "\n",
    "data_df_predict = data_df_predict.drop(columns=columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run                  numeric- 13 unique value(s)\n",
      "Source                                   (Data Type: object) - 4 unique value(s)\n",
      "Purity                                   (Data Type: object) - 14 unique value(s)\n",
      "MIDS                 numeric- 24 unique value(s)\n",
      "TotalReads(M)        numeric- 95 unique value(s)\n",
      "lpWGSReads(M)        numeric- 86 unique value(s)\n",
      "TargetPanelReads(M)  numeric- 64 unique value(s)\n",
      "%ReadslpWGS                              (Data Type: object) - 36 unique value(s)\n",
      "%ReadsPanel                              (Data Type: object) - 36 unique value(s)\n",
      "1000x                                    (Data Type: object) - 65 unique value(s)\n",
      "500x                                     (Data Type: object) - 56 unique value(s)\n",
      "200x                                     (Data Type: object) - 32 unique value(s)\n",
      "100x                                     (Data Type: object) - 16 unique value(s)\n",
      "50x                                      (Data Type: object) - 10 unique value(s)\n",
      "25x                                      (Data Type: object) - 6 unique value(s)\n",
      "DupFrac                                  (Data Type: object) - 36 unique value(s)\n",
      "LowCovRegions        numeric- 51 unique value(s)\n",
      "PurityPloidyRatio                        (Data Type: object) - 20 unique value(s)\n",
      "ResNoise                                 (Data Type: object) - 23 unique value(s)\n",
      "SignalNoiseRatio                         (Data Type: object) - 113 unique value(s)\n",
      "QAStatus                                 (Data Type: object) - 3 unique value(s)\n",
      "Gene                                     (Data Type: object) - 4 unique value(s)\n",
      "Variant                                  (Data Type: object) - 39 unique value(s)\n",
      "%VariantFraction                         (Data Type: object) - 39 unique value(s)\n",
      "MyriadGIScore        numeric- 70 unique value(s)\n",
      "MyriadGIStatus       numeric- 2 unique value(s)\n",
      "SOPHiAGIIndex                            (Data Type: object) - 105 unique value(s)\n",
      "SophiaGIStatus       numeric- 4 unique value(s)\n"
     ]
    }
   ],
   "source": [
    "# Finding attribute columns\n",
    "application_categories = data_df.dtypes[data_df.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "# Check the number of unique values in each column of object type\n",
    "columns = data_df[application_categories].nunique()\n",
    "\n",
    "# Iterate through the columns and print the unique value counts for each column\n",
    "# we iterate through every column in the dataframe, some of them of object type where we found the unique count\n",
    "for column in data_df.columns:\n",
    "    if column in columns.index:\n",
    "        data_type = data_df[column].dtype\n",
    "        print(f\"{column.ljust(40)} (Data Type: {data_type}) - {columns[column]} unique value(s)\")\n",
    "    else:\n",
    "        print(f\"{column.ljust(20)} numeric- {data_df[column].nunique()} unique value(s)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity    object\n",
      "dtype: object\n",
      "Purity    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert columns we will not bin, from pobject type to numeric\n",
    "column_names_to_convert = ['Purity']\n",
    "\n",
    "# Step 1: Check the current data types of the columns\n",
    "print(data_df[column_names_to_convert].dtypes)\n",
    "\n",
    "# Step 2: Convert each column to numeric (if possible)\n",
    "for col in column_names_to_convert:\n",
    "    data_df[col] = pd.to_numeric(data_df[col], errors='coerce')\n",
    "    data_df_predict[col] = pd.to_numeric(data_df_predict[col], errors='coerce') \n",
    "# Step 3: Check the new data types of the columns after the conversion\n",
    "print(data_df[column_names_to_convert].dtypes)\n",
    "# print(data_df[\"Source\"].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['81%' '76%' '64%' '63%' '58%' '54%' '90%' '89%' '84%' '85%' '98%' '72%'\n",
      " '68%' '69%' '74%' '66%' '73%' '70%' '80%' '86%' '15%' '91%' '62%' '79%'\n",
      " '88%' '92%' '71%' '77%' '59%' '60%' '55%' '65%' '67%' '57%' '61%' '75%']\n",
      "['19%' '24%' '36%' '37%' '42%' '46%' '10%' '12%' '16%' '15%' '2%' '28%'\n",
      " '32%' '31%' '26%' '29%' '34%' '25%' '30%' '20%' '14%' '85%' '9%' '38%'\n",
      " '21%' '11%' '8%' '23%' '41%' '40%' '27%' '45%' '35%' '33%' '43%' '39%']\n"
     ]
    }
   ],
   "source": [
    "print(data_df['%ReadslpWGS'].unique())\n",
    "print(data_df['%ReadsPanel'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "data_df['PurityPloidyRatio'] = data_df['PurityPloidyRatio'].replace('-', 0.0)\n",
    "data_df['ResNoise'] = data_df['ResNoise'].replace('-', 0.0)\n",
    "data_df['SignalNoiseRatio'] = data_df['SignalNoiseRatio'].replace('-', 0.0)\n",
    "\n",
    "data_df['Gene'] = data_df['Gene'].replace('.', 'Unlisted')\n",
    "data_df_predict['Gene'] = data_df_predict['Gene'].fillna('Unlisted')\n",
    "\n",
    "# data_df['MonthsOld'] = data_df['MonthsOld'].fillna(0.0)\n",
    "# data_df['MonthsOld'] = data_df['MonthsOld'].replace('.', 0.0)\n",
    "data_df['Purity'] = data_df['Purity'].replace('.', 0.0)\n",
    "data_df['%VariantFraction'] = data_df['%VariantFraction'].replace('.', 0.0)\n",
    "data_df_predict['%VariantFraction'] = data_df_predict['%VariantFraction'].replace('',0.0)\n",
    "# Convert the 'Purity' column to numeric, replacing '.' with 0.0\n",
    "data_df['Purity'] = pd.to_numeric(data_df['Purity'], errors='coerce').fillna(0.0)\n",
    "\n",
    "data_df['DupFrac'] = data_df['DupFrac'].replace('%', '', regex=True).astype(float)\n",
    "data_df['%ReadslpWGS'] = data_df['%ReadslpWGS'].replace('%', '', regex=True).astype(float)\n",
    "data_df['%ReadsPanel'] = data_df['%ReadsPanel'].replace('%', '', regex=True).astype(float)\n",
    "\n",
    "data_df['Variant'] = data_df['Variant'].replace('.', 'Unlisted')\n",
    "\n",
    "data_df_predict['Variant'] = data_df_predict['Variant'].fillna('Unlisted')\n",
    "# Apply label encoding to 'Variant' column\n",
    "label_encoder = LabelEncoder()\n",
    "data_df['Variant'] = label_encoder.fit_transform(data_df['Variant'].astype(str))\n",
    "data_df_predict['Variant'] = label_encoder.fit_transform(data_df_predict['Variant'].astype(str))\n",
    "\n",
    "data_df_predict['Run'] = data_df_predict['Run'].str.replace('Run', '')\n",
    "data_df_predict['MIDS'] = data_df_predict['MIDS'].str.replace('S', '')\n",
    "\n",
    "data_df['1000x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "\n",
    "data_df['500x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "data_df['200x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "data_df['100x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "data_df['50x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "data_df['25x'] = data_df['1000x'].replace('%', '', regex=True).astype(float)\n",
    "\n",
    "data_df_predict['MyriadGIScore'] = '0'\n",
    "data_df_predict['MyriadGIStatus'] = '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Run  Source  Purity  MIDS  TotalReads(M)  lpWGSReads(M)  \\\n",
      "0      1      AZ    20.0     2            7.3            5.9   \n",
      "1      1      AZ    30.0     3            7.3            5.6   \n",
      "2      1      AZ    20.0     4            9.6            6.1   \n",
      "3      1      AZ    20.0     6            8.9            5.6   \n",
      "4      1      AZ    60.0     7            8.6            5.0   \n",
      "..   ...     ...     ...   ...            ...            ...   \n",
      "134    6      AZ    10.0     3           25.5           18.8   \n",
      "135   11  GREECE     0.0     4           18.6           13.0   \n",
      "136    2      AZ    40.0    23           18.3           12.2   \n",
      "137    5      AZ    50.0    23           14.0           10.5   \n",
      "138   12  BRAZIL    60.0    23           21.6           15.5   \n",
      "\n",
      "     TargetPanelReads(M)  %ReadslpWGS  %ReadsPanel  1000x  ...  \\\n",
      "0                    1.4         81.0         19.0    1.0  ...   \n",
      "1                    1.7         76.0         24.0    2.0  ...   \n",
      "2                    3.5         64.0         36.0   41.0  ...   \n",
      "3                    3.3         63.0         37.0   16.0  ...   \n",
      "4                    3.6         58.0         42.0    2.0  ...   \n",
      "..                   ...          ...          ...    ...  ...   \n",
      "134                  6.7         74.0         26.0   88.0  ...   \n",
      "135                  5.6         70.0         30.0    0.0  ...   \n",
      "136                  6.1         66.0         34.0   10.0  ...   \n",
      "137                  3.5         75.0         25.0    1.0  ...   \n",
      "138                  6.1         72.0         28.0   82.0  ...   \n",
      "\n",
      "     SignalNoiseRatio  QAStatus      Gene  Variant  %VariantFraction  \\\n",
      "0                2.95    Medium  Unlisted        0               0.0   \n",
      "1                2.91      High  Unlisted        0               0.0   \n",
      "2                1.64      High  Unlisted        0               0.0   \n",
      "3                3.49      High  Unlisted        0               0.0   \n",
      "4                2.18      High  Unlisted        0               0.0   \n",
      "..                ...       ...       ...      ...               ...   \n",
      "134              1.08    Medium  Unlisted        0               0.0   \n",
      "135               0.4    Medium  Unlisted        0               0.0   \n",
      "136              1.56    Medium  Unlisted        0               0.0   \n",
      "137               1.7    Medium     BRCA2       26              26.7   \n",
      "138               1.1    Medium     BRCA1       25              64.7   \n",
      "\n",
      "     MyriadGIScore  MyriadGIStatus SOPHiAGIIndex SophiaGIStatus Non-agreement  \n",
      "0               51               1           3.2              1             1  \n",
      "1               20               2         -15.7              2             1  \n",
      "2               17               2          -4.6              2             1  \n",
      "3               29               2          -4.6              2             1  \n",
      "4               29               2          -8.2              2             1  \n",
      "..             ...             ...           ...            ...           ...  \n",
      "134              3               2            -9              2             1  \n",
      "135              7               2             -              3             0  \n",
      "136             38               2          -4.9              2             1  \n",
      "137             25               2          -1.4              2             1  \n",
      "138             31               2          -2.7              2             1  \n",
      "\n",
      "[139 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "def calculate_non_agreement(row):\n",
    "    if row['MyriadGIStatus'] == 1 and row['SophiaGIStatus'] == 1:\n",
    "        return 1\n",
    "    elif row['MyriadGIStatus'] == 1 and row['SophiaGIStatus'] == 2:\n",
    "        return 0\n",
    "    elif row['MyriadGIStatus'] == 1 and row['SophiaGIStatus'] == 3:\n",
    "        return 0\n",
    "    elif row['MyriadGIStatus'] == 1 and row['SophiaGIStatus'] == 4:\n",
    "        return 0\n",
    "    elif row['MyriadGIStatus'] == 2 and row['SophiaGIStatus'] == 1:\n",
    "        return 0\n",
    "    elif row['MyriadGIStatus'] == 2 and row['SophiaGIStatus'] == 2:\n",
    "        return 1\n",
    "    elif row['MyriadGIStatus'] == 2 and row['SophiaGIStatus'] == 3:\n",
    "        return 0\n",
    "    elif row['MyriadGIStatus'] == 2 and row['SophiaGIStatus'] == 4:\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the 'Non-agreement' column\n",
    "data_df['Non-agreement'] = data_df.apply(calculate_non_agreement, axis=1)\n",
    "data_df_predict['Non-agreement'] = 'void'\n",
    "#print(data_df['Non-agreement'].dtypes)\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Run  Source  Purity  MIDS  TotalReads(M)  lpWGSReads(M)  \\\n",
      "0      1      AZ    20.0     2            7.3            5.9   \n",
      "1      1      AZ    30.0     3            7.3            5.6   \n",
      "2      1      AZ    20.0     4            9.6            6.1   \n",
      "3      1      AZ    20.0     6            8.9            5.6   \n",
      "4      1      AZ    60.0     7            8.6            5.0   \n",
      "..   ...     ...     ...   ...            ...            ...   \n",
      "134    6      AZ    10.0     3           25.5           18.8   \n",
      "135   11  GREECE     0.0     4           18.6           13.0   \n",
      "136    2      AZ    40.0    23           18.3           12.2   \n",
      "137    5      AZ    50.0    23           14.0           10.5   \n",
      "138   12  BRAZIL    60.0    23           21.6           15.5   \n",
      "\n",
      "     TargetPanelReads(M)  %ReadslpWGS  %ReadsPanel  1000x  ...  \\\n",
      "0                    1.4         81.0         19.0    1.0  ...   \n",
      "1                    1.7         76.0         24.0    2.0  ...   \n",
      "2                    3.5         64.0         36.0   41.0  ...   \n",
      "3                    3.3         63.0         37.0   16.0  ...   \n",
      "4                    3.6         58.0         42.0    2.0  ...   \n",
      "..                   ...          ...          ...    ...  ...   \n",
      "134                  6.7         74.0         26.0   88.0  ...   \n",
      "135                  5.6         70.0         30.0    0.0  ...   \n",
      "136                  6.1         66.0         34.0   10.0  ...   \n",
      "137                  3.5         75.0         25.0    1.0  ...   \n",
      "138                  6.1         72.0         28.0   82.0  ...   \n",
      "\n",
      "     SignalNoiseRatio  QAStatus      Gene  Variant  %VariantFraction  \\\n",
      "0                2.95    Medium  Unlisted        0               0.0   \n",
      "1                2.91      High  Unlisted        0               0.0   \n",
      "2                1.64      High  Unlisted        0               0.0   \n",
      "3                3.49      High  Unlisted        0               0.0   \n",
      "4                2.18      High  Unlisted        0               0.0   \n",
      "..                ...       ...       ...      ...               ...   \n",
      "134              1.08    Medium  Unlisted        0               0.0   \n",
      "135               0.4    Medium  Unlisted        0               0.0   \n",
      "136              1.56    Medium  Unlisted        0               0.0   \n",
      "137               1.7    Medium     BRCA2       26              26.7   \n",
      "138               1.1    Medium     BRCA1       25              64.7   \n",
      "\n",
      "     MyriadGIScore  MyriadGIStatus SOPHiAGIIndex SophiaGIStatus Non-agreement  \n",
      "0               51               1           3.2              1             1  \n",
      "1               20               2         -15.7              2             1  \n",
      "2               17               2          -4.6              2             1  \n",
      "3               29               2          -4.6              2             1  \n",
      "4               29               2          -8.2              2             1  \n",
      "..             ...             ...           ...            ...           ...  \n",
      "134              3               2            -9              2             1  \n",
      "135              7               2             -              3             0  \n",
      "136             38               2          -4.9              2             1  \n",
      "137             25               2          -1.4              2             1  \n",
      "138             31               2          -2.7              2             1  \n",
      "\n",
      "[139 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  QAStatus  COUNT\n",
      "0     High     89\n",
      "1   Medium     42\n",
      "2      Low      8\n",
      "---\n",
      "       Gene  COUNT\n",
      "0  Unlisted     99\n",
      "1     BRCA1     21\n",
      "2     BRCA2     18\n",
      "3    RAD51D      1\n",
      "---\n",
      "    Variant  COUNT\n",
      "0         0     99\n",
      "1        14      2\n",
      "2        21      2\n",
      "3        36      1\n",
      "4        27      1\n",
      "5        22      1\n",
      "6        23      1\n",
      "7        24      1\n",
      "8        25      1\n",
      "9        37      1\n",
      "10       26      1\n",
      "11       28      1\n",
      "12       35      1\n",
      "13       20      1\n",
      "14       30      1\n",
      "15       31      1\n",
      "16       32      1\n",
      "17       33      1\n",
      "18       34      1\n",
      "19       29      1\n",
      "20       19      1\n",
      "21        1      1\n",
      "22       18      1\n",
      "23        2      1\n",
      "24        3      1\n",
      "25        4      1\n",
      "26        5      1\n",
      "27        6      1\n",
      "28        7      1\n",
      "29        8      1\n",
      "30        9      1\n",
      "31       10      1\n",
      "32       11      1\n",
      "33       12      1\n",
      "34       13      1\n",
      "35       15      1\n",
      "36       16      1\n",
      "37       17      1\n",
      "38       38      1\n",
      "---\n",
      "   PurityPloidyRatio  COUNT\n",
      "0                0.0     46\n",
      "1               0.23     15\n",
      "2               0.25     11\n",
      "3               0.45      9\n",
      "4                0.3      8\n",
      "5               0.28      7\n",
      "6               0.33      6\n",
      "7                0.2      5\n",
      "8               0.35      5\n",
      "9               0.38      5\n",
      "10              0.15      4\n",
      "11                 0      3\n",
      "12              0.42      3\n",
      "13              0.47      3\n",
      "14              0.17      2\n",
      "15              0.12      2\n",
      "16               0.4      2\n",
      "17               0.1      1\n",
      "18              0.07      1\n",
      "19               0.5      1\n"
     ]
    }
   ],
   "source": [
    "# Look at APPLICATION_TYPE value counts for binning  QAStatus\n",
    "\n",
    "grouped_df = data_df.groupby(\"QAStatus\").size().reset_index(name=\"COUNT\")\n",
    "sorted_df = grouped_df.sort_values(by=\"COUNT\", ascending=False)\n",
    "sorted_df = sorted_df.reset_index(drop=True)\n",
    "print(sorted_df)\n",
    "print('---')\n",
    "grouped_df = data_df.groupby(\"Gene\").size().reset_index(name=\"COUNT\")\n",
    "sorted_df = grouped_df.sort_values(by=\"COUNT\", ascending=False)\n",
    "sorted_df = sorted_df.reset_index(drop=True)\n",
    "print(sorted_df)\n",
    "print('---')\n",
    "grouped_df = data_df.groupby(\"Variant\").size().reset_index(name=\"COUNT\")\n",
    "sorted_df = grouped_df.sort_values(by=\"COUNT\", ascending=False)\n",
    "sorted_df = sorted_df.reset_index(drop=True)\n",
    "print(sorted_df)\n",
    "print('---')\n",
    "grouped_df = data_df.groupby(\"PurityPloidyRatio\").size().reset_index(name=\"COUNT\")\n",
    "sorted_df = grouped_df.sort_values(by=\"COUNT\", ascending=False)\n",
    "sorted_df = sorted_df.reset_index(drop=True)\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dummy block of code in case we want to gather many column values into the same bucket\n",
    "# application_types_to_replace = [\"T9\", \"T13\", \"T12\", \"T2\", \"T14\", \"T25\", \"T29\", \"T15\", \"T17\"]\n",
    "\n",
    "# Replace the specified values in the \"APPLICATION_TYPE\" column with \"Other\"\n",
    "# data_df['APPLICATION_TYPE'] = data_df['APPLICATION_TYPE'].replace(application_types_to_replace, \"Other\")\n",
    "\n",
    "# Check the value counts after replacing\n",
    "# print(data_df['APPLICATION_TYPE'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run                      int64\n",
      "Source                  object\n",
      "Purity                 float64\n",
      "MIDS                     int64\n",
      "TotalReads(M)          float64\n",
      "lpWGSReads(M)          float64\n",
      "TargetPanelReads(M)    float64\n",
      "%ReadslpWGS            float64\n",
      "%ReadsPanel            float64\n",
      "1000x                  float64\n",
      "500x                   float64\n",
      "200x                   float64\n",
      "100x                   float64\n",
      "50x                    float64\n",
      "25x                    float64\n",
      "DupFrac                float64\n",
      "LowCovRegions            int64\n",
      "PurityPloidyRatio       object\n",
      "ResNoise                object\n",
      "SignalNoiseRatio        object\n",
      "QAStatus                object\n",
      "Gene                    object\n",
      "Variant                  int32\n",
      "%VariantFraction        object\n",
      "MyriadGIScore            int64\n",
      "MyriadGIStatus           int64\n",
      "SOPHiAGIIndex           object\n",
      "SophiaGIStatus           int64\n",
      "Non-agreement            int64\n",
      "Variant                  int32\n",
      "QAStatus_High            uint8\n",
      "QAStatus_Low             uint8\n",
      "QAStatus_Medium          uint8\n",
      "Gene_BRCA1               uint8\n",
      "Gene_BRCA2               uint8\n",
      "Gene_RAD51D              uint8\n",
      "Gene_Unlisted            uint8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create one-hot columns for these 3 columns\n",
    "onehot_cols = [\"QAStatus\", \"Gene\", \"Variant\"]  #\"PurityPloidyRatio\"]\n",
    "\n",
    "# Use get_dummies() to one-hot encode only the categorical columns\n",
    "one_hot_encoded = pd.get_dummies(data_df[onehot_cols])\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "data_df = pd.concat([data_df, one_hot_encoded], axis=1)\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(data_df_predict[onehot_cols])\n",
    "data_df_predict = pd.concat([data_df_predict, one_hot_encoded], axis=1 )\n",
    "# After this, you can print the data types of columns in the 'data_df' DataFrame\n",
    "column_types = data_df.dtypes\n",
    "print(column_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_callback(message):\n",
    "    print(f\"Final callback: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_accuracy_callback(epoch, loss, accuracy):\n",
    "    print(f\"Epoch {epoch}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from datetime import datetime\n",
    "\n",
    "def main_process(callback, X_train_scaled, y_train, nn, report_interval=5):\n",
    "    # Train the model and store the training history\n",
    "    fit_model = nn.fit(X_train_scaled, y_train, epochs=100, verbose=0, callbacks=[callback])  # Pass the callback here\n",
    "    training_history = fit_model.history\n",
    "    \n",
    "    print(\"Training has started.\")\n",
    "    epochs = 100\n",
    "    \n",
    "    # Report loss and accuracy at the specified intervals\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        if epoch % report_interval == 0 or epoch == epochs:\n",
    "            loss = training_history['loss'][epoch - 1]\n",
    "            accuracy = training_history['accuracy'][epoch - 1]\n",
    "            print(f\"Epoch {epoch}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\")#soos2\n",
    "            callback.on_epoch_end(epoch, {'loss': loss, 'accuracy': accuracy})  # Manually call the on_epoch_end method\n",
    "\n",
    "    result = \"Task completed.\"\n",
    "    # final_callback(result)  # Comment out or remove this line as it is not defined in the code\n",
    "    print(\"Main process finished.\")\n",
    "\n",
    "# soos\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "script_name = \"Starter_Codev1\"\n",
    "csv_loss_accuracy_file = \"startercodetest.csv\"\n",
    "#csv_loss_accuracy_file_name = f\"{script_name}_test_result_{current_datetime}.csv\"\n",
    "\n",
    "# Create the CSVLogger callback and pass the file name\n",
    "csv_logger = CSVLogger(csv_loss_accuracy_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "### In our original model, we created data frame application_df, which still exists.\n",
    "- 1. I will create a pca method on this data frame which has had bucketing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Run', 'Source', 'Purity', 'MIDS', 'TotalReads(M)', 'lpWGSReads(M)',\n",
       "       'TargetPanelReads(M)', '%ReadslpWGS', '%ReadsPanel', '1000x', '500x',\n",
       "       '200x', '100x', '50x', '25x', 'DupFrac', 'LowCovRegions',\n",
       "       'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio', 'Variant',\n",
       "       '%VariantFraction', 'Variant', 'QAStatus_High', 'QAStatus_Low',\n",
       "       'QAStatus_Medium', 'Gene_BRCA1', 'Gene_BRCA2', 'Gene_RAD51D',\n",
       "       'Gene_Unlisted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1  pca method on application_df which has had bucketing performed for the original model.\n",
    "data_df_x = data_df.copy() \n",
    "columns_to_drop = [\"Gene\", \"Non-agreement\", \"MyriadGIStatus\", \"SophiaGIStatus\", \"MyriadGIScore\", \"SOPHiAGIIndex\", \"QAStatus\"]\n",
    "data_df_x = data_df_x.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "data_df_predict_x = data_df_predict.copy()\n",
    "data_df_predict_x = data_df_predict_x.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "#data_df_x = data_df_x.drop(columns=[\"Gene\",\"Source\",\"Non_agreement\", \"MyriadGIStatus\", \"SophiaGIStatus\", \"MyriadGIScore\", \"SOPHiAGIIndex\",\"QAStatus\"], axis=1)\n",
    "data_df_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Run', 'Source', 'Purity', 'MIDS', 'TotalReads(M)', 'lpWGSReads(M)',\n",
       "       'TargetPanelReads(M)', '%ReadslpWGS', '%ReadsPanel', '1000x', '500x',\n",
       "       '200x', '100x', '50x', '25x', 'DupFrac', 'LowCovRegions',\n",
       "       'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio', 'Variant',\n",
       "       '%VariantFraction', 'Variant', 'QAStatus_High', 'QAStatus_Medium',\n",
       "       'Gene_BRCA1', 'Gene_BRCA2', 'Gene_Unlisted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_predict_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: bool)\n"
     ]
    }
   ],
   "source": [
    "#data_df_x\n",
    "# Step 1: Check which columns have missing values (NaN)\n",
    "columns_with_missing_values = data_df_x.isna().any()\n",
    "\n",
    "# Step 2: Output the columns where the count of missing values is larger than 0\n",
    "columns_with_missing_values = columns_with_missing_values[columns_with_missing_values]\n",
    "\n",
    "# Output the columns with missing values\n",
    "print(columns_with_missing_values)\n",
    "#print(data_df_x['%ReadslpWGS'].unique())\n",
    "#print(data_df_x['%ReadsPanel'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0 '42.6' '78.1' '78.5' '77.2' '22.1' '37.8' '56.4' '24.4' '87' '54.6'\n",
      " '51.1' '85.8' '71.8' '66.1' '9' '87.4' '84.1' '80.4' '73' '67.6' '17.3'\n",
      " '9.1' '35.8' '82.8' '80.5' 'Deleted' '18.7' '65.3' '51.8' '68.8' '62.6'\n",
      " '71.2' '89.8' '47' '59.6' '5.4' '26.7' '64.7']\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the \"%VariantFraction\" column\n",
    "print(data_df['%VariantFraction'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Deleted\" with 0 in the '%ReadslpWGS' column\n",
    "data_df_x['%VariantFraction'] = data_df_x['%VariantFraction'].replace('Deleted', 0)\n",
    "\n",
    "# Convert the column to numeric (float) format\n",
    "data_df_x['%VariantFraction'] = pd.to_numeric(data_df_x['%VariantFraction'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns:\n",
      "Index(['Source', 'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#data_df_x.columns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data_df_x' is the DataFrame containing the data\n",
    "non_numeric_columns = data_df_x.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "print(\"Non-numeric columns:\")\n",
    "print(non_numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Source', 'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "non_numeric_columns = data_df_x.select_dtypes(exclude=[np.number]).columns\n",
    "print(non_numeric_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run                      int64\n",
      "Source                  object\n",
      "Purity                 float64\n",
      "MIDS                     int64\n",
      "TotalReads(M)          float64\n",
      "lpWGSReads(M)          float64\n",
      "TargetPanelReads(M)    float64\n",
      "%ReadslpWGS            float64\n",
      "%ReadsPanel            float64\n",
      "1000x                  float64\n",
      "500x                   float64\n",
      "200x                   float64\n",
      "100x                   float64\n",
      "50x                    float64\n",
      "25x                    float64\n",
      "DupFrac                float64\n",
      "LowCovRegions            int64\n",
      "PurityPloidyRatio       object\n",
      "ResNoise                object\n",
      "SignalNoiseRatio        object\n",
      "Variant                  int32\n",
      "%VariantFraction       float64\n",
      "Variant                  int32\n",
      "QAStatus_High            uint8\n",
      "QAStatus_Low             uint8\n",
      "QAStatus_Medium          uint8\n",
      "Gene_BRCA1               uint8\n",
      "Gene_BRCA2               uint8\n",
      "Gene_RAD51D              uint8\n",
      "Gene_Unlisted            uint8\n",
      "dtype: object\n",
      "Run                    0\n",
      "Source                 0\n",
      "Purity                 0\n",
      "MIDS                   0\n",
      "TotalReads(M)          0\n",
      "lpWGSReads(M)          0\n",
      "TargetPanelReads(M)    0\n",
      "%ReadslpWGS            0\n",
      "%ReadsPanel            0\n",
      "1000x                  0\n",
      "500x                   0\n",
      "200x                   0\n",
      "100x                   0\n",
      "50x                    0\n",
      "25x                    0\n",
      "DupFrac                0\n",
      "LowCovRegions          0\n",
      "PurityPloidyRatio      0\n",
      "ResNoise               0\n",
      "SignalNoiseRatio       0\n",
      "Variant                0\n",
      "%VariantFraction       0\n",
      "Variant                0\n",
      "QAStatus_High          0\n",
      "QAStatus_Low           0\n",
      "QAStatus_Medium        0\n",
      "Gene_BRCA1             0\n",
      "Gene_BRCA2             0\n",
      "Gene_RAD51D            0\n",
      "Gene_Unlisted          0\n",
      "dtype: int64\n",
      "Run                      0\n",
      "Source                 139\n",
      "Purity                   0\n",
      "MIDS                     0\n",
      "TotalReads(M)            0\n",
      "lpWGSReads(M)            0\n",
      "TargetPanelReads(M)      0\n",
      "%ReadslpWGS              0\n",
      "%ReadsPanel              0\n",
      "1000x                    0\n",
      "500x                     0\n",
      "200x                     0\n",
      "100x                     0\n",
      "50x                      0\n",
      "25x                      0\n",
      "DupFrac                  0\n",
      "LowCovRegions            0\n",
      "PurityPloidyRatio        0\n",
      "ResNoise                 0\n",
      "SignalNoiseRatio         0\n",
      "Variant                  0\n",
      "%VariantFraction         0\n",
      "Variant                  0\n",
      "QAStatus_High            0\n",
      "QAStatus_Low             0\n",
      "QAStatus_Medium          0\n",
      "Gene_BRCA1               0\n",
      "Gene_BRCA2               0\n",
      "Gene_RAD51D              0\n",
      "Gene_Unlisted            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check data types of the columns in data_df_x\n",
    "print(data_df_x.dtypes)\n",
    "\n",
    "# Check for missing values in data_df_x\n",
    "print(data_df_x.isnull().sum())\n",
    "\n",
    "# Convert the '%VariantFraction' column to numeric values, invalid values will be converted to NaN\n",
    "data_df_x['%VariantFraction'] = pd.to_numeric(data_df_x['%VariantFraction'], errors='coerce')\n",
    "\n",
    "# Create a mask to identify rows with NaN values in the '%VariantFraction' column\n",
    "invalid_rows_mask = data_df_x['%VariantFraction'].isna()\n",
    "\n",
    "# Use the mask to filter the DataFrame and get the rows with invalid values\n",
    "invalid_rows = data_df_x[invalid_rows_mask]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if any non-numeric values still exist in data_df_x\n",
    "non_numeric_values = data_df_x.apply(pd.to_numeric, errors='coerce').isnull().sum()\n",
    "print(non_numeric_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: Index(['Source', 'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio'], dtype='object')\n",
      "Missing values:\n",
      " Run                    0\n",
      "Source                 0\n",
      "Purity                 0\n",
      "MIDS                   0\n",
      "TotalReads(M)          0\n",
      "lpWGSReads(M)          0\n",
      "TargetPanelReads(M)    0\n",
      "%ReadslpWGS            0\n",
      "%ReadsPanel            0\n",
      "1000x                  0\n",
      "500x                   0\n",
      "200x                   0\n",
      "100x                   0\n",
      "50x                    0\n",
      "25x                    0\n",
      "DupFrac                0\n",
      "LowCovRegions          0\n",
      "PurityPloidyRatio      0\n",
      "ResNoise               0\n",
      "SignalNoiseRatio       0\n",
      "Variant                0\n",
      "%VariantFraction       0\n",
      "Variant                0\n",
      "QAStatus_High          0\n",
      "QAStatus_Low           0\n",
      "QAStatus_Medium        0\n",
      "Gene_BRCA1             0\n",
      "Gene_BRCA2             0\n",
      "Gene_RAD51D            0\n",
      "Gene_Unlisted          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for non-numeric columns\n",
    "non_numeric_cols = data_df_x.select_dtypes(exclude=[np.number]).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data_df_x.isnull().sum()\n",
    "print(\"Missing values:\\n\", missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Run', 'Purity', 'MIDS', 'TotalReads(M)', 'lpWGSReads(M)',\n",
       "       'TargetPanelReads(M)', '%ReadslpWGS', '%ReadsPanel', '1000x', '500x',\n",
       "       '200x', '100x', '50x', '25x', 'DupFrac', 'LowCovRegions',\n",
       "       'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio', '%VariantFraction',\n",
       "       'QAStatus_High', 'QAStatus_Low', 'QAStatus_Medium', 'Gene_BRCA1',\n",
       "       'Gene_BRCA2', 'Gene_RAD51D', 'Gene_Unlisted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['Variant', 'Source']\n",
    "data_df_x = data_df_x.drop(columns=columns_to_drop)\n",
    "\n",
    "data_df_predict_x = data_df_predict_x.drop(columns=columns_to_drop)\n",
    "\n",
    "data_df_x.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_predict_x.columns\n",
    "data_df_predict_x['QAStatus_Low'] = 0\n",
    "data_df_predict_x['Gene_RAD51D'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Run, Purity, MIDS, TotalReads(M), lpWGSReads(M), TargetPanelReads(M), %ReadslpWGS, %ReadsPanel, 1000x, 500x, 200x, 100x, 50x, 25x, DupFrac, LowCovRegions, PurityPloidyRatio, ResNoise, SignalNoiseRatio, %VariantFraction, QAStatus_High, QAStatus_Low, QAStatus_Medium, Gene_BRCA1, Gene_BRCA2, Gene_RAD51D, Gene_Unlisted]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "invalid_rows = data_df_x['%VariantFraction'].apply(lambda x: not str(x).replace('.', '').isnumeric())\n",
    "\n",
    "# Display the rows containing the invalid values\n",
    "print(data_df_x[invalid_rows])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139, 27)\n",
      "(111, 27) (28, 27)\n",
      "(111, 3) (28, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume data_df_x contains the features, and data_df contains the target \"Non_agreement\" column\n",
    "\n",
    "# Split our preprocessed data into our features and target arrays\n",
    "y2 = data_df[\"Non-agreement\"].values\n",
    "X2 = data_df_x.values\n",
    "X3 = data_df_predict_x.values\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=78)\n",
    "\n",
    "# Create a new scaler instance and fit it on the training data\n",
    "scaler = StandardScaler()\n",
    "X2_train_scaled = scaler.fit_transform(X2_train)\n",
    "\n",
    "# Transform both the training and testing data using the same scaler\n",
    "X2_test_scaled = scaler.transform(X2_test)\n",
    "X3_scaled = scaler.transform(X3)\n",
    "# Now perform PCA on the scaled training data\n",
    "pca = PCA(n_components=3)\n",
    "X2_train_pca = pca.fit_transform(X2_train_scaled)\n",
    "\n",
    "# Apply the same PCA transformation to the scaled testing data\n",
    "X2_test_pca = pca.transform(X2_test_scaled)\n",
    "\n",
    "# Check the number of records in the original dataset\n",
    "print(data_df_x.shape)\n",
    "\n",
    "# Check the number of records after splitting into training and testing sets\n",
    "print(X2_train.shape, X2_test.shape)\n",
    "\n",
    "# Check the number of records after PCA transformation\n",
    "print(X2_train_pca.shape, X2_test_pca.shape)\n",
    "\n",
    "# Create a new DataFrame with the PCA data for both training and testing sets\n",
    "df_train_pca = pd.DataFrame(X2_train_pca, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "df_test_pca = pd.DataFrame(X2_test_pca, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer instance with the strategy as 'constant' and fill_value as 0\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "# Impute missing values in X3_scaled with 0\n",
    "X3_imputed = imputer.fit_transform(X3_scaled)\n",
    "\n",
    "# Perform PCA on the imputed data\n",
    "X3_pca = pca.transform(X3_imputed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate my PCA bucketted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 6)                 24        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 56        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "number_input_features = 3  # Adjust this to match the PCA-transformed dimension\n",
    "hidden_nodes_layer1 = 6    # Define the number of nodes in the first hidden layer\n",
    "hidden_nodes_layer2 = 8    # Define the number of nodes in the second hidden layer\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"tanh\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "nn.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Summary of the model architecture\n",
    "nn.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "#  YOUR CODE GOES HERE\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started.\n",
      "Epoch 5: Loss=0.7300, Accuracy=0.4865\n",
      "Epoch 10: Loss=0.6528, Accuracy=0.6667\n",
      "Epoch 15: Loss=0.6012, Accuracy=0.7838\n",
      "Epoch 20: Loss=0.5688, Accuracy=0.8198\n",
      "Epoch 25: Loss=0.5465, Accuracy=0.8018\n",
      "Epoch 30: Loss=0.5304, Accuracy=0.8108\n",
      "Epoch 35: Loss=0.5173, Accuracy=0.8108\n",
      "Epoch 40: Loss=0.5067, Accuracy=0.8108\n",
      "Epoch 45: Loss=0.4976, Accuracy=0.8108\n",
      "Epoch 50: Loss=0.4903, Accuracy=0.8108\n",
      "Epoch 55: Loss=0.4838, Accuracy=0.8108\n",
      "Epoch 60: Loss=0.4782, Accuracy=0.8108\n",
      "Epoch 65: Loss=0.4737, Accuracy=0.8198\n",
      "Epoch 70: Loss=0.4694, Accuracy=0.8198\n",
      "Epoch 75: Loss=0.4654, Accuracy=0.8288\n",
      "Epoch 80: Loss=0.4621, Accuracy=0.8288\n",
      "Epoch 85: Loss=0.4591, Accuracy=0.8288\n",
      "Epoch 90: Loss=0.4563, Accuracy=0.8288\n",
      "Epoch 95: Loss=0.4538, Accuracy=0.8288\n",
      "Epoch 100: Loss=0.4518, Accuracy=0.8288\n",
      "Main process finished.\n"
     ]
    }
   ],
   "source": [
    "# Call main_process function\n",
    "main_process(csv_logger, X2_train_pca, y2_train, nn, report_interval=5)\n",
    "#main_process(csv_logger, df_train_pca, y2_train, nn, report_interval=5)\n",
    "#main_process(csv_logger, X2_train_scaled, y2_train, nn, report_interval=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'class_name': 'InputLayer', 'config': {'batch_input_shape': [None, 43], 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'dense_input'}}, {'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'batch_input_shape': [None, 43], 'dtype': 'float32', 'units': 8, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 5, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define the script name manually (replace 'Your_Script_Name' with the actual name of your script)\n",
    "script_name = \"PCAProject4_Codev1\"\n",
    "\n",
    "# Generate the output file name with the script name and date/time\n",
    "output_file_name = f\"{script_name}_{current_datetime}.h5\"\n",
    "\n",
    "# Save the model with the date and time in the file name\n",
    "nn.save(output_file_name)\n",
    "\n",
    "# Output CSV file name with the script name and date/time\n",
    "csv_file_name = f\"{script_name}_model_data_{current_datetime}.csv\"\n",
    "json_name = f\"{script_name}_model_data_{current_datetime}.json\"\n",
    "# Extract model configuration (architecture and hyperparameters)\n",
    "model_config = nn.get_config()\n",
    "\n",
    "# Write the model information to the CSV file\n",
    "with open(csv_file_name, \"w\", newline=\"\") as csv_file:\n",
    "    fieldnames = model_config.keys()  # Retrieve model configuration keys\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()  # Write header with field names\n",
    "    writer.writerow(model_config)  # Write the model configuration to the CSV\n",
    "\n",
    "# Corrected JSON-like string\n",
    "corrected_json_str = '[{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 43], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"batch_input_shape\": [null, 43], \"dtype\": \"float32\", \"units\": 8, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 5, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]'\n",
    "\n",
    "# Convert the JSON-like string to a list of dictionaries\n",
    "layers_list = json.loads(corrected_json_str)\n",
    "\n",
    "with open(json_name, \"w\") as json_file:\n",
    "    json.dump(layers_list, json_file)\n",
    "\n",
    "# Print the list of dictionaries\n",
    "print(layers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.5915 - accuracy: 0.7143 - 226ms/epoch - 226ms/step\n",
      "Loss: 0.5914893746376038, Accuracy: 0.7142857313156128\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def write_test_result_to_csv(file_name, loss, accuracy, current_datetime):\n",
    "    with open(file_name, \"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"Metric\", \"Value\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()  # Write header with field names\n",
    "        writer.writerow({\"Metric\": \"Loss\", \"Value\": loss})\n",
    "        writer.writerow({\"Metric\": \"Accuracy\", \"Value\": accuracy})\n",
    "        writer.writerow({\"Metric\": \"Date\", \"Value\": current_datetime})  # Write current date and time\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "# model_loss, model_accuracy = nn.evaluate(X2_test_scaled, y2_test, verbose=2)\n",
    "model_loss, model_accuracy = nn.evaluate(X2_test_pca, y2_test, verbose=2)\n",
    "\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "# Output CSV file name for test results with the script name and date/time\n",
    "#test_result_csv_file = f\"{script_name}_loss_accuracy_{current_datetime}.csv\"\n",
    "test_result_csv_file = \"test_results.csv\"\n",
    "# Write the test result to the CSV file with the current date and time\n",
    "write_test_result_to_csv(test_result_csv_file, model_loss, model_accuracy, current_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def csv_to_json(csv_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(csv_file_path, 'r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            data.append(row)\n",
    "\n",
    "    return data\n",
    "\n",
    "csv_file_path = 'test_results.csv'\n",
    "\n",
    "# Convert CSV data to JSON format\n",
    "json_data = csv_to_json(csv_file_path)\n",
    "\n",
    "\n",
    "json_file_path = f\"{script_name}_loss_accuracy_{current_datetime}.json\"\n",
    "# Write the JSON data to a JSON file\n",
    "# json_file_path = 'test_results.json'\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "csv_file_path = 'startercodetest.csv'\n",
    "\n",
    "# Convert CSV data to JSON format\n",
    "json_data = csv_to_json(csv_file_path)\n",
    "\n",
    "# Write the JSON data to a JSON file\n",
    "# json_file_path = 'startercodetest.json'\n",
    "json_file_path = f\"{script_name}_test_result_{current_datetime}.json\"\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 114ms/step\n",
      "[[ 0  8]\n",
      " [ 0 20]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have a trained model named 'nn' and test data 'X2_test_pca', 'y2_test'\n",
    "predictions = nn.predict(X2_test_pca)\n",
    "\n",
    "# Convert the probability predictions to class labels\n",
    "predictions = (predictions > 0.5).astype(int)  # Assuming the threshold is 0.5 for binary classification\n",
    "\n",
    "# Choose the class for which you want to calculate the binary confusion matrix\n",
    "positive_class = 1\n",
    "\n",
    "# Create binary labels for the positive class\n",
    "y_true_binary = (y2_test == positive_class).astype(int)\n",
    "\n",
    "# Calculate the binary confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_binary, predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "print(conf_matrix)\n",
    "\n",
    "confusion_csv_file = f\"{script_name}_confusion_matrix_{current_datetime}.csv\"\n",
    "\n",
    "np.savetxt(confusion_csv_file, conf_matrix, delimiter=\",\", fmt='%.2f')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Run', 'Purity', 'MIDS', 'TotalReads(M)', 'lpWGSReads(M)',\n",
       "       'TargetPanelReads(M)', '%ReadslpWGS', '%ReadsPanel', '1000x', '500x',\n",
       "       '200x', '100x', '50x', '25x', 'DupFrac', 'LowCovRegions',\n",
       "       'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio', '%VariantFraction',\n",
       "       'QAStatus_High', 'QAStatus_Low', 'QAStatus_Medium', 'Gene_BRCA1',\n",
       "       'Gene_BRCA2', 'Gene_RAD51D', 'Gene_Unlisted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'PurityPloidyRatio' does not exist in the test data. Setting default value to -1.\n",
      "Column 'ResNoise' does not exist in the test data. Setting default value to -1.\n",
      "Column 'SignalNoiseRatio' does not exist in the test data. Setting default value to -1.\n",
      "[( 4, 70.,  4, 24. , 21.7, 2.3, 90., 10.,  0.,  0.,  0.,  0.,  0.,  0., 75., 14, 0.  , 0.13, 2.6 , 85.8, 0, 0, 1, 0, 1, 0, 0, 1)\n",
      " (12, 70.,  9, 17. , 11.6, 5.4, 68., 32., 67., 67., 67., 67., 67., 67., 66.,  0, 0.28, 0.14, 1.66,  0. , 1, 0, 0, 0, 0, 0, 1, 1)\n",
      " ( 6, 70., 11, 14.1, 10.3, 3.8, 73., 27.,  5.,  5.,  5.,  5.,  5.,  5., 81., 34, 0.35, 0.23, 1.35,  0. , 1, 0, 0, 0, 0, 0, 1, 1)\n",
      " (11,  0.,  6, 20.3, 14.1, 6.2, 70., 30., 96., 96., 96., 96., 96., 96., 62.,  0, 0.1 , 0.06, 2.14,  0. , 1, 0, 0, 0, 0, 0, 1, 1)\n",
      " (11,  0., 18, 18.4, 14.9, 3.5, 81., 19., 12., 12., 12., 12., 12., 12., 65.,  3, 0.  , 0.07, 1.96,  0. , 0, 0, 1, 0, 0, 0, 1, 1)]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Convert the non-numeric columns with 'NA' values to -1\n",
    "non_numeric_columns = ['PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio']\n",
    "for col in non_numeric_columns:\n",
    "    try:\n",
    "        column_idx = np.where(X2_test.dtype.names == col)[0][0]\n",
    "        X2_test[col] = np.where(X2_test[col] == 'NA', -1, X2_test[col])\n",
    "    except IndexError:\n",
    "        print(f\"Column '{col}' does not exist in the test data. Setting default value to -1.\")\n",
    "\n",
    "# Concatenate the processed X_test and y_test data\n",
    "data_to_save = np.column_stack((X2_test, y2_test))\n",
    "\n",
    "column_headers = ['Run', 'Purity', 'MIDS', 'TotalReads(M)', 'lpWGSReads(M)',\n",
    "                  'TargetPanelReads(M)', '%ReadslpWGS', '%ReadsPanel', '1000x', '500x',\n",
    "                  '200x', '100x', '50x', '25x', 'DupFrac', 'LowCovRegions',\n",
    "                  'PurityPloidyRatio', 'ResNoise', 'SignalNoiseRatio', '%VariantFraction',\n",
    "                  'QAStatus_High', 'QAStatus_Low', 'QAStatus_Medium', 'Gene_BRCA1',\n",
    "                  'Gene_BRCA2', 'Gene_RAD51D', 'Gene_Unlisted','Y-Predict']\n",
    "\n",
    "\n",
    "\n",
    "# Create a filename with the current datetime\n",
    "#current_datetime = datetime.datetime. now().strftime(\"%Y%m%d%H%M%S\")\n",
    "#csv_file_name = f\"processed_test_data_{current_datetime}.csv\"\n",
    "csv_file_name = \"processed_test_data.csv\"\n",
    "data_with_headers = np.vstack((column_headers, data_to_save))\n",
    "\n",
    "\n",
    "\n",
    "# Save the concatenated data to the file\n",
    "np.savetxt(csv_file_name, data_with_headers, delimiter=',', fmt='%s')\n",
    "# np.savetxt(csv_file_name, data_to_save, delimiter=',', fmt='%s')\n",
    "\n",
    "\n",
    "def csv_to_json(csv_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(csv_file_path, 'r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            data.append(row)\n",
    "\n",
    "    return data\n",
    "\n",
    "csv_file_path = 'processed_test_data.csv'\n",
    "\n",
    "# Convert CSV data to JSON format\n",
    "json_data = csv_to_json(csv_file_path)\n",
    "\n",
    "json_file_path = f\"{script_name}_processed_test_data{current_datetime}.json\"\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the saved data from the file and print the first few rows\n",
    "saved_data = np.genfromtxt(csv_file_name, delimiter=',', dtype=None, names=True, encoding=None)\n",
    "print(saved_data[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions_x3 = nn.predict(X3_pca)\n",
    "#predictions_df = pd.DataFrame(predictions_x3)\n",
    "predictions_df = pd.DataFrame(predictions_x3, columns=['Predictions'])\n",
    "# Reset the column names of predictions_df to None\n",
    "#predictions_df.columns = [None] * len(predictions_df.columns)\n",
    "\n",
    "# Create the DataFrame x4 from X3\n",
    "x4 = pd.DataFrame(X3)\n",
    "#x4.columns = [None] * len(x4.columns)\n",
    "# Concatenate X3 and predictions_df horizontally\n",
    "x4 = pd.concat([x4, predictions_df], axis=1)\n",
    "\n",
    "# Save x4 to a CSV file\n",
    "x4.to_csv('x4_predictions.csv', index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48]\n",
      " [0.42]\n",
      " [0.74]\n",
      " [0.71]\n",
      " [0.72]\n",
      " [0.7 ]\n",
      " [0.74]\n",
      " [0.66]\n",
      " [0.35]\n",
      " [0.63]\n",
      " [0.7 ]\n",
      " [0.68]\n",
      " [0.7 ]\n",
      " [0.7 ]\n",
      " [0.66]\n",
      " [0.71]\n",
      " [0.5 ]\n",
      " [0.69]\n",
      " [0.47]\n",
      " [0.73]\n",
      " [0.33]\n",
      " [0.49]\n",
      " [0.69]\n",
      " [0.32]\n",
      " [0.68]\n",
      " [0.47]\n",
      " [0.36]\n",
      " [0.75]\n",
      " [0.43]\n",
      " [0.73]\n",
      " [0.67]\n",
      " [0.68]\n",
      " [0.37]\n",
      " [0.77]\n",
      " [0.32]\n",
      " [0.53]\n",
      " [0.79]\n",
      " [0.47]\n",
      " [0.72]\n",
      " [0.33]\n",
      " [0.72]\n",
      " [0.38]\n",
      " [0.72]\n",
      " [0.72]\n",
      " [0.45]\n",
      " [0.66]\n",
      " [0.52]\n",
      " [0.46]\n",
      " [0.77]\n",
      " [0.7 ]\n",
      " [0.7 ]\n",
      " [0.51]\n",
      " [0.71]\n",
      " [0.71]\n",
      " [0.71]\n",
      " [0.76]\n",
      " [0.36]\n",
      " [0.71]\n",
      " [0.75]\n",
      " [0.42]\n",
      " [0.67]\n",
      " [0.45]\n",
      " [0.67]\n",
      " [0.3 ]\n",
      " [0.71]\n",
      " [0.72]\n",
      " [0.67]\n",
      " [0.36]\n",
      " [0.74]\n",
      " [0.48]\n",
      " [0.71]\n",
      " [0.68]\n",
      " [0.5 ]\n",
      " [0.71]\n",
      " [0.49]\n",
      " [0.82]\n",
      " [0.76]\n",
      " [0.8 ]\n",
      " [0.41]\n",
      " [0.73]\n",
      " [0.68]\n",
      " [0.69]\n",
      " [0.66]\n",
      " [0.74]\n",
      " [0.65]\n",
      " [0.72]\n",
      " [0.77]\n",
      " [0.69]\n",
      " [0.44]\n",
      " [0.71]\n",
      " [0.73]\n",
      " [0.72]\n",
      " [0.7 ]\n",
      " [0.35]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions_x3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
